id1,id2,paperAbstract1,paperAbstract2,label
482bd1fe-16ad-41f7-b398-691fb9cf3d01,7518cf2d-23a7-4b85-94ae-984c5c8aaba6,"As a novel way to protect connected cars, we are developing a Security Information and Event Management System (SIEM) called Security Management of Services in Connected Cars (SeMaCoCa) located in the backend of the connected car. For that we defined a connected car architecture and possible use cases which serve as a basis for the research. Using data from the connected cars and additional information, attacks on individual vehicles or fleets should be recognized. A combination of rule-based-, machine-learning-, deep learning-, real-time-based-, security-algorithms and algorithms for big data are used. Furthermore, we aim for a privacy-friendly solution that does not require the backend operator to have access to cleartext data. The new security system should be able to recognise misbehaviour under the conditions of a permanently growing number and variety of connected cars, upcoming services on the market and related constantly to changing user behavior. The challenge for the security system is, that under these conditions no stable system state exists, that the system can rely on. In this paper, we introduce the architecture of SeMaCoCa, user stories and the idea behind the approach of the system.","The emergence of advanced technologies such as InfiniBand and Non-Volatile Memory together with decreasing in DRAM prices has enabled us to build ultra-low latency in-memory stores for backing real-time and large-scale services. Many studies have taken those opportunities to accelerate lookup operations of key-value stores. Meanwhile, there is a lack of researches that utilize them for improving range query performance. This paper reports a detailed performance analysis of range query execution of in-memory stores. Our findings show that in modern architecture, copying/serialization data for transmission over network is the major overhead of range query processing. To solve this issue, several methods could be considered, such as parallelism, caching, and utilizing RDMA Read. We have conducted some experiments to evaluate their potentials and surprisingly, the outcomes revealed that none of those performed well in every scenario. Optimizing performance gain when employing those techniques requires us to carefully address user behaviors, characteristics of data and even system architecture.",0
3b229d0d-f0c7-4059-84b9-337aa16c4ff2,12dcb823-df0c-4699-8925-4b75bae5818a,"Metamorphic malware changes its internal structure on each infection while maintaining its function. Although many detection techniques have been proposed, practical and effective metamorphic detection remains a difficult challenge. In this paper, we analyze a previously proposed eigenvector-based method for metamorphic detection. The approach considered here was inspired by a well-known facial recognition technique. We compute eigenvectors using raw byte data extracted from executables belonging to a metamorphic family. These eigenvectors are then used to compute a score for a collection of executable files that includes family viruses and representative examples of benign code. We perform extensive testing to determine the effectiveness of this classification method. Among other results, we show that this eigenvalue-based approach is effective when applied to a family of highly metamorphic code that successfully evades statistical-based detection. We also experiment computing eigenvectors on extracted opcode sequences, as opposed to raw byte sequences. Our experimental evidence indicates that the use of opcode sequences does not improve the results.","In this paper, we consider a method for computing the similarity of executable files, based on opcode graphs. We apply this technique to the challenging problem of metamorphic malware detection and compare the results to previous work based on hidden Markov models. In addition, we analyze the effect of various morphing techniques on the success of our proposed opcode graph-based detection scheme.",1
3c44cc43-dbeb-4164-86df-a55fb7208ce5,1813231e-a7d7-4850-9234-8c328d556693,"When screen reader users need to back track pages to re-find previously visited content, they are forced to listen to some portion of each unwanted page to recognize it. This makes aural back navigation inefficient, especially on large websites. To address this problem, we introduce topic- and list-based back: two navigation strategies that provide back browsing shortcuts by leveraging the conceptual structure of content-rich websites. Both are manifested in Webtime, an accessible website on the history of the Web. A controlled study (N=10) conducted at the Indiana School for the Blind and Visually Impaired compared topic- and list-based back to traditional back mechanisms while participants completed fact-finding tasks. Topic- and list-based back significantly decreased time-on-task and number of backtracked pages; the navigation shortcuts were also associated with positive improvements in perceived cognitive effort and navigation experience. The proposed strategies can operate as a supplement to current back mechanisms in information-rich websites.",,1
6d72c90a-8965-4a97-9e50-203a9b83bef7,1a0748e8-7784-4443-85ce-24b0bb7b10dc,"One of the most challenging decisions that a manager must confront is whether to continue or abandon a troubled project. Published studies suggest that failing software projects are often allowed to continue for too long before appropriate management action is taken to discontinue or redirect the efforts. The level of sunk cost associated with such projects has been offered as one explanation for this escalation of commitment behavior. What prior studies fail to consider is how concepts from risk-taking theory (such as risk propensity and risk perception) affect decision makers' willingness to continue a project under conditions of sunk cost. To better understand factors that may cause decision makers to continue such projects, this study examines the level of sunk cost together with the risk propensity and risk perception of decision makers. These factors are assessed for cross-cultural robustness using matching laboratory experiments carried out in three cultures (Finland, the Netherlands, and Singapore). With a wider set of explanatory factors than prior studies, we could account for a higher amount of variance in decision makers' willingness to continue a project. The level of sunk cost and the risk perception of decision makers contributed significantly to their willingness to continue a project. Moreover, the risk propensity of decision makers was inversely related to risk perception. This inverse relationship was significantly stronger in Singapore (a low uncertainty avoidance culture) than in Finland and the Netherlands (high uncertainty avoidance cultures). These results reveal that some factors behind decision makers' willingness to continue a project are consistent across cultures while others may be culture-sensitive. Implications of these results for further research and practice are discussed.",,1
11eee561-1cf1-4649-b40d-a2259f2c5b4d,1b791ab6-1187-48b0-a72f-b589b4ed1592,We present the novel boomerang protocol to efficiently retain information at a particular geographic location in a sparse network of highly mobile nodes without use of infrastructure networks. Our proof-of-concept implementation revealed the main challenge in implementing the Boomerang protocol is to accurately detect whether a node is divergent from a recorded trajectory and then followed up with a detailed study to address the challenge. Simulation with automotive traffic traces for a southern New Jersey region shows that the protocol improves packet return rate by 70% compared to a baseline implementation using shortest path geographic routing.,"Vehicular sensor networks are emerging as a new network paradigm of primary relevance, especially for proactively gathering monitoring information in urban environments. Vehicles typically have no strict constraints on processing power and storage capabilities. They can sense events (e.g., imaging from streets), process sensed data (e.g., recognizing license plates), and route messages to other vehicles (e.g., diffusing relevant notification to drivers or police agents). In this novel and challenging mobile environment, sensors can generate a sheer amount of data, and traditional sensor network approaches for data reporting become unfeasible. This article proposes MobEyes, an efficient lightweight support for proactive urban monitoring based on the primary idea of exploiting vehicle mobility to opportunistically diffuse summaries about sensed data. The reported experimental/analytic results show that MobEyes can harvest summaries and build a low-cost distributed index with reasonable completeness, good scalability, and limited overhead",1
f9d8a719-7a9a-40c3-bbd3-483df1a75ccc,1dafe571-674f-458e-8663-4d4501d05e16,"As an extension to the current Web, Semantic Web will not only contain structured data with machine understandable semantics but also textual information. While structured queries can be used to find information more precisely on the Semantic Web, keyword searches are still needed to help exploit textual information. It thus becomes very important that we can combine precise structured queries with imprecise keyword searches to have a hybrid query capability. In addition, due to the huge volume of information on the Semantic Web, the hybrid query must be processed in a very scalable way. In this paper, we define such a hybrid query capability that combines unary tree-shaped structured queries with keyword searches. We show how existing information retrieval (IR) index structures and functions can be reused to index semantic web data and its textual information, and how the hybrid query is evaluated on the index structure using IR engines in an efficient and scalable manner. We implemented this IR approach in an engine called Semplore. Comprehensive experiments on its performance show that it is a promising approach. It leads us to believe that it may be possible to evolve current web search engines to query and search the Semantic Web. Finally, we breifly describe how Semplore is used for searching Wikipedia and an IBM customer's product information.","Swoogle is a crawler-based indexing and retrieval system for the Semantic Web. It extracts metadata for each discovered document, and computes relations between documents. Discovered documents are also indexed by an information retrieval system which can use either character N-Gram or URIrefs as keywords to find relevant documents and to compute the similarity among a set of documents. One of the interesting properties we compute is  ontology rank , a measure of the importance of a Semantic Web document.",1
aaa00b28-3c6a-41f1-92d6-24b73acc6ac5,1e07eeee-1cd0-4dbe-92bc-3f3f17c9d033,"Mass transport ticketing with mobile phones is already deployed in many metropolitan areas, but current solutions and protocols are not secure, and they are limited to one-time or fixed-time ticketing in non-gated transport systems. The emergence of NFC-enabled phones with trusted execution environments makes it possible to not only integrate mobile phone ticketing with existing and future transport authority ticket readers, but also to construct secure protocols for non-gated travel eliminating many associated possibilities for ticketing fraud. This paper presents an architecture and implementation for such a system.","A theme of recent side-channel research has been the quest for distinguishers which remain effective even when few assumptions can be made about the underlying distribution of the measured leakage traces. The Kolmogorov-Smirnov (KS) test is a well known non-parametric method for distinguishing between distributions, and, as such, a perfect candidate and an interesting competitor to the (already much discussed) mutual information (MI) based attacks. However, the side-channel distinguisher based on the KS test statistic has received only cursory evaluation so far, which is the gap we narrow here. This contribution explores the effectiveness and efficiency of Kolmogorov-Smirnov analysis (KSA), and compares it with mutual information analysis (MIA) in a number of relevant scenarios ranging from optimistic first-order DPA to multivariate settings. We show that KSA shares certain â€˜generic' capabilities in common with MIA whilst being more robust to noise than MIA in univariate settings. This has the practical implication that designers should consider results of KSA to determine the resilience of their designs against univariate power analysis attacks.",1
1df8263b-adb0-4a1b-8c6b-27b6d2c7be89,238a4c4c-4327-463d-86e0-2a4714e3c3e1,"Multi-Processor Systems-on-Chips (MPSoCs) emerge as the predominant platform in embedded real-time applications. A large variety of ubiquitous services should be implemented by embedded systems in a cost- and power-efficient way, yet providing a maximum degree of performance, usability and dependability. By using a scalable Network-on-Chip (NoC) architecture which replaces the traditional point-to-point and bus connections in conjunction with performant IP cores it is possible to use the available performance to consolidate functionality on a single MPSoC platform. But especially when uncritical best-effort applications (e.g., entertainment) and critical applications (e.g., pedestrian detection, electronic stability control) are combined on the same architecture (mixed-criticality), validation faces new challenges. Due to complex resource sharing in MPSoCs the timing behavior becomes more complex and requires new analysis methods. Additionally, applications that may exhibit multiple behaviors corresponding to different operating modes (e.g., initialization mode, fault-recovery mode) need to be also considered in the design of mixed-critical MPSoCs. In this paper, challenges in the design of mixed-critical systems are discussed and formal analysis solutions which consider shared resources, NoC communication, multi-mode applications and their reliabilities are proposed.","Checkpointing is a relatively cost effective method for achieving fault tolerance in real-time systems. Since checkpointing schemes depend on time redundancy, they could affect the correctness of the system by causing deadlines to be missed. This paper provides exact schedulability tests for fault tolerant task sets under specified failure hypothesis and employing checkpointing to assist in fault recovery. The effects of checkpointing strategies on task response time are analysed and some insights for optimal checkpointing are provided. The emphasis here is on utilizing this analysis as an off-line design support tool.",1
2f1e3528-2b48-4920-8fbf-4ba2b2ac260a,27c3c741-4f38-430f-80ee-06cec8278cbe,"In this paper, a novel model of object-based visual attention extending Duncan's Integrated Competition Hypothesis [Phil. Trans. R. Soc. London B 353 (1998) 1307-1317] is presented. In contrast to the attention mechanisms used in most previous machine vision systems which drive attention based on the spatial location hypothesis, the mechanisms which direct visual attention in our system are object-driven as well as feature-driven. The competition to gain visual attention occurs not only within an object but also between objects. For this purpose, two new mechanisms in the proposed model are described and analyzed in detail. The first mechanism computes the visual salience of objects and groupings; the second one implements the hierarchical selectivity of attentional shifts. The results of the new approach on synthetic and natural images are reported.","Abstract   We describe a novel dynamic and multiresolution attention scheme for the generation of visual saccades and its application to locate candidate regions for facial feature recognition. The low-level, data-driven attention model suggested herein, employs a nonlinear sampling lattice of oriented Gaussian filters and uses small oscillatory movements to extract local image characteristics (conspicuity). As the sampling grid moves over the image, multiresolution ``evidences'' of local features are accumulated in a short-term visual memory. We propose a simple integration technique that computes the saliency surface iteratively across saccadic movements. Simulation results on face images demonstrate the applicability of our approach.",1
25761c27-673b-4ccd-93d4-ad21e8ea6841,2a99782e-5a72-40d3-8574-7918e1415ba5,"Engaging users in threat reporting is important in order to improve threat monitoring in urban environments. Today, mobile applications are mostly used to provide basic reporting interfaces. With a rapid evolution of mobile devices, the idea of context awareness has gained a remarkable popularity in recent years. Modern smartphones and tablets are equipped with a variety of sensors including accelerometers, gyroscopes, pressure gauges, light and GPS sensors. Additionally, the devices become computationally powerful which allows for real-time processing of data gathered by their sensors. Universal access to the Internet via WiFi hot-spots and GSM network makes mobile devices perfect platforms for ubiquitous computing. Although there exist numerous frameworks for context-aware systems, they are usually dedicated to static, centralized, client-server architectures. There is still space for research in the field of context modeling and reasoning for mobile devices. In this paper, we propose a lightweight context-aware framework for mobile devices that uses data gathered by mobile device sensors and performs on-line reasoning about possible threats based on the information provided by the Social Threat Monitor system developed in the INDECT project.","Context-aware pervasive systems are emerging as an important class of applications. Such systems can respond intelligently to contextual information about the physical world acquired via sensors and information about the computational environment. A declarative approach to building context-aware pervasive systems is presented, and the notion of the situation program is introduced, which highlights the primacy of the situation abstraction for building context-aware pervasive systems. There is also a demonstration of how to manipulate situation programs using meta-programming within an extension of the Prolog logic programming language which is called LogicCAP. Such meta-reasoning enables complex situations to be described in terms of other situations. Furthermore, a discussion is given on how the design of situation programs can affect the properties of a context-aware system. The approach encourages a high-level of abstraction for representing and reasoning with situations, and supports building context-aware systems incrementally by providing modularity and separation of concerns.",1
11997fa1-0571-413a-bacd-4ae3162353dd,72b43f43-1284-485a-8b0c-89a167a6ba7a,"Data analytics frameworks shift towards larger degrees of parallelism. Efficient scheduling of data-parallel jobs (tasks) is critical for improving job performance such as response time, and resource utilization. It is an important challenge for large scale data analytics frameworks in which jobs are more complex and have diverse characteristics (e.g., diverse resource requirements). Prior work on scheduling cannot achieve low response time and high resource utilization simultaneously because they cannot accurately estimate the durations of tasks in the queue of a worker machine by using sampling-based approach (including sampling with late binding) for task placement, and thus they fail to place tasks at the best possible worker machine. Also, they do not sufficiently consider the diverse resource requirements of jobs (tasks) for placing tasks on worker machines. To address this challenge, we propose a Dependency-aware and Resource-efficient Scheduling (DRS) to achieve low response time and high resource utilization. DRS takes into account task dependency and assigns tasks that are independent of each other to different worker machines. Also, DRS considers tasks' resource requirements and packs complementary tasks whose resource demands on multiple resources are complementary to each other to increase the resource utilization. In addition, DRS uses the mutual reinforcement learning to estimate the task's waiting time (the duration of tasks in the queue of a worker), and assigns tasks to workers with the consideration of tasks' waiting time to reduce the response time. Extensive experimental results based on a real cluster and experiments using real-world Amazon EC2 cloud service show that DRS achieves low response time and high resource utilization compared to previous strategies.","The focus of this contribution is to review delta-sigma based all-digital transmitters and to discuss issues related to large out-of-band quantization noise and possible coexistence problems. Low-complexity embedded-FIR filters are very interesting to relax the filtering constraints while keeping systems as digital as possible to benefit from the advanced CMOS node integration. In this paper we propose a single-bit digital to RF mixer with embedded-FIR, which provides noise level reduction at specific frequencies in order to target multi-standard coexistence. This architecture introduces simple logic operating at low frequency which enables single-bit output and avoids the use of an additional delayed DAC, thus reducing considerably the power consumption and area of the output stage. Finally, we introduce an asymmetric unbalanced FIR architecture to provide a complementary solution for out-of-band noise reduction.",0
8ce9618b-adb1-4815-8082-f580511a76ff,2e90c9d6-0422-4472-8a2e-03f75bcbbf60,"Wi-Fi has been widely adopted by families in Taiwan, for it has the advantage of designing family monitoring systems with its wide usage. Therefore, there are many advantages for designing family monitoring systems. Based on our previous research architecture, we implemented a monitoring system for family environments based on Wi-Fi and sensor technologies. Humidity, gas, smoke, and temperature sensors are applied for designing and developing the intelligent home environment monitoring system in this paper. A system prototype is proposed in this research, and the experimental results are discussed. The results showed that the system is workable and remarkable for environment monitoring. The contribution of this is to propose a framework for future home environment monitoring system developers to refer to.","The new generation of multimedia surveillance systems integrates a large number of heterogeneous sensors to collect, process, and analyze multimedia data for identifying events of potential security threats. Some of the major concerns facing these systems are scalability, ubiquitous access to sensory data, event processing overhead, and massive storage requirementsâ€”all of which demand novel scalable approach. Cloud computing can provide a powerful and scalable infrastructure for large-scale storage, processing, and dissemination of sensor data. Furthermore, the integration of sensor technology and cloud computing offers new possibilities for efficient development and deployment of sensor-based systems. This paper proposes a framework for a cloud-based multimedia surveillance system and highlights several research and technical issues. A prototype surveillance system is also designed and analyzed in the context of the proposed surveillance framework. The paper finally reports that cloud-based multimedia surveillance system can effectively support the processing overload, storage requirements, ubiquitous access, security, and privacy in large-scale surveillance settings.",1
f788d49d-1a4c-4bda-948f-1bce02be1bec,2ecc37b1-7541-405d-b9a6-75560217d768,"A phrase unit speech recognition system is discussed, which is applicable for a large vocabulary and is independent of the task. In the case of large vocabulary, it is desirable to express the words in the dictionary by the sequence of phonemes or phoneme-like units. Therefore, the recognition of phonemes in continuous speech is essential to achieve a flexible speech understanding system. In this paper, a technique to recognize phrases based on the phoneme recognition is introduced. The system is composed of the phoneme recognition part and the phrase recognition part. In the phoneme recognition part, the features in the articulatory domain are extracted and applied to compensate coarticulation. In the phrase recognition part, a word sequence corresponding to the phoneme sequence is determined by using two-level DP matching with automaton control, in which words are processed symbolically to attain the acceptable processing speed.","Articulatory parameters estimated from speech waves were used for the recognition of semivowels and consonants in continuous speech. It has been shown that introduction of the articulatory model in speech recognition is one effective method to solve the difficulties of coarticulation phenomena and speaker differences. In this paper, the recognition of semivowels and consonants is discussed. As for semivowels, it is found that the phase difference between the movement of the tongue and that of the jaw is important to characterize semivowels, and this can be effectively used in the recognition. In the case of consonants, it is possible to find the typical feature of each consonant which corresponds to its place of articulation in the transient parts of the articulatory parameters. A preliminary experiment adopting the DP matching technique in VCV contexts gave fairly hopeful results. And for nasal sounds, it is shown that introduction of the nasal model is useful. The nasal model consists of the nasal cavity and the velum parameter.",1
9d213453-a85c-4617-9790-aca33cad7365,fdf3310e-feea-4f92-9ca9-6ee68dc7d143,"This paper presents an adaptive audio watermarking algorithm in the wavelet domain to optimize the payload under the perceptual transparency constraints of audio signal by strategically using some of its local features. Unlike existing algorithms, the watermark payload in this approach is made adaptive based on the nature of the audio signal. This localized feature based approach to determine the payload addresses the issue of over-loading and under-loading the audio signals with watermark data making the payload optimized for each individual audio host signal. Some audio features are strategically extracted and the most discriminatory features are selected using Principal Component analysis (PCA) approach. A mathematical model is designed using selected audio features like energy, zero cross mean and short time energy to evaluate the degree of embedding under perceptual transparency. It is used to estimate the number of watermarking bits to be inserted for a particular audio signal which makes the approach adaptive in nature optimizing the watermarking payload. At the embedding stage, watermark is embedded in the host audio signal in the third level detailed coefficient of wavelet domain which strikes a balance between the contradicting design parameters of perceptual transparency, robustness and optimized payload. Watermark extraction in this paper is blind with good robustness to signal processing attacks. Experimental results validate that the proposed adaptive algorithm provide good imperceptibility with good robustness against signal processing attacks at adjustable payload for different types of audio signals. Comparative analysis indicates that this proposed adaptive algorithm has better performance in terms of imperceptibility and robustness in comparison to uniform watermarking algorithm.","We consider the problem of selecting an optimal set of sensors to estimate the states of linear dynamical systems. Specifically, the goal is to choose (at design-time) a subset of sensors (satisfying certain budget constraints) from a given set in order to minimize the trace of the steady state  a priori  or  a posteriori  error covariance produced by a Kalman filter. We show that the  a priori  and  a posteriori  error covariance-based sensor selection problems are both NP-hard, even under the additional assumption that the system is stable. We then provide bounds on the worst-case performance of sensor selection algorithms based on the system dynamics, and show that greedy algorithms are optimal for a certain class of systems. However, as a negative result, we show that certain typical objective functions are not submodular or supermodular in general. While this makes it difficult to evaluate the performance of greedy algorithms for sensor selection (outside of certain special cases), we show via simulations that these greedy algorithms perform well in practice.",0
04a1f767-2306-4cb4-aff0-456b985f4c28,33c7f69a-0d93-4a66-8297-d71f0e069232,"Bipartite Correlation clustering is the problem of generating a set of disjoint bi-cliques on a set of nodes while minimizing the symmetric difference to a bipartite input graph. The number or size of the output clusters is not constrained in any way. The best known approximation algorithm for this problem gives a factor of 11. This result and all previous ones involve solving large linear or semi-definite programs which become prohibitive even for modestly sized tasks. In this paper we present an improved factor 4 approximation algorithm to this problem using a simple combinatorial algorithm which does not require solving large convex programs. The analysis extends a method developed by Ailon, Charikar and Alantha in 2008, where a randomized pivoting algorithm was analyzed for obtaining a 3-approximation algorithm for Correlation Clustering, which is the same problem on graphs (not bipartite). The analysis for Correlation Clustering there required defining events for structures containing 3 vertices and using the probability of these events to produce a feasible solution to a dual of a certain natural LP bounding the optimal cost. It is tempting here to use sets of 4 vertices, which are the smallest structures for which contradictions arise for Bipartite Correlation Clustering. This simple idea, however, appears to be evasive. We show that, by modifying the LP, we can analyze algorithms which take into consideration subgraph structures of unbounded size. We believe our techniques are interesting in their own right, and may be used for other problems as well.","The NP-hard BICLUSTER EDITING is to add or remove at most k edges to make a bipartite graph G = (V, E) a vertex-disjoint union of complete bipartite subgraphs. It has applications in the analysis of gene expression data. We show that by polynomial-time preprocessing, one can shrink a problem instance to one with 4k vertices, thus proving that the problem has a linear kernel, improving a quadratic kernel result. We further give a search tree algorithm that improves the running time bound from the trivial O(4k + |E|) to O(3.24k + |E|). Finally, we give a randomized 4-approximation, improving a known approximation with factor 11.",1
346dbbaa-dc78-46fb-8acc-20ecca97f5df,379b73a4-fded-4921-8af9-13f6823b54c6,"ABSTRACT#R##N##R##N#A personal network (PN) should enable the collaboration of user's devices and services in a flexible, self-organizing, and friendly manner. For such purpose, the PN must securely accommodate heterogeneous technologies with uneven computational and communication resources. In particular, personal radio frequency identification (RFID) tags can enable seamless recognition of user's context, provide user authentication, and enable novel services enhancing the quality and quantity of data handled by the PN. However, the highly constrained features of common RFID tags and their passive role in the network highlights the need of an adequate secure communication model with personal tags, which enables their participation as a member of the PN. In this paper, we present our concept of PN, with special emphasis on the role of RFID and sensor networks, and define a secure architecture for PNs including methods for the secure access to context-aware technologies from both local PN members and the Internet of Things. The PN architecture is designed to support differentiated security mechanisms to maximize the level of security for each type of personal device. Furthermore, we analyze which security solutions available in the literature can be adapted for our architecture, as well as the challenges and security mechanisms still necessary in the secure integration of personal tags. Copyright Â© 2013 John Wiley & Sons, Ltd.","Identity Management has so far been a field mainly applications and Web focused. This paper describes a novel approach to cross layer identity management that extends digital identities to the network, the virtual identity (VID) framework. The VID framework provides strong privacy to the user, while easily supporting personalization cross-service providers. While other identity management solutions are tailored to one specific application and/or protocol domain, the proposed framework extends the use of one's digital identity to all aspects of the network and services architecture. It is also the first to consider legal constrains, such as ownership of data and legal intercept issues, in such a broad scope. One major aspect reported here is the relevance for operators.",1
252f22ca-f05c-4c4a-bed2-1a9609b098da,3bc34af7-ab06-4271-9a7a-052a45b42bf5,"A two-dimensional framework is proposed as a basis for assessing users' self-report responses to website designs. This incorporates two features that have been consistently identified in the psychology and design literatures: (a) a processing sequence and (b) a distinction between cognition and affect. Suggested advantages include increased clarity with regard to the identification of self-report constructs, stronger links to relevant literature, and improved â€œactionabilityâ€ù of designs. To examine this framework empirically, a study was conducted for which participants were required to imagine selecting a psychology university degree program and to provide a series of ratings of the designs of five department websites. The proposed framework enabled the identification of distinct and interpretable patterns of users' responses. In this regard, a number of consistencies with recent conceptualizations of user experience were noted. Implications and limitations of the proposed approach were considered.","Recent research into user experience has identified the need for a theoretical model to build cumulative knowledge in research addressing how the overall quality or 'goodness' of an interactive product is formed. An experiment tested and extended Hassenzahl's model of aesthetic experience. The study used a 2x2x(2) experimental design with three factors: principles of screen design, principles for organizing information on a web page and experience of using a web site. Dependent variables included hedonic perceptions and evaluations of a web site as well as measures of task performance, navigation behaviour and mental effort. Measures, except Beauty, were sensitive to manipulation of web design. Beauty was influenced by hedonic attributes (identification and stimulation), but Goodness by both hedonic and pragmatic (user-perceived usability) attributes as well as task performance and mental effort. Hedonic quality was more stable with experience of web-site use than pragmatic quality and Beauty was more stable than Goodness.",1
c01fcb0d-03ec-4e86-9640-9f66ae4c7122,3bc34af7-ab06-4271-9a7a-052a45b42bf5,"This article presents an evaluation framework for user preference research. Web application was implemented to verify this framework. Interrelated influence of three main aspects of the website - usability, aesthetics and information part - is demonstrated in a performed experiment. Surprisingly, the perception of aesthetics was most significantly influenced by the quality of two remaining factors. The original hypothesis was, that the quality of aesthetics will influence the perception of usability and information part. The article also features a parallel comparison approach, which allows within-subject design and two-way performed manipulation of variables.","Recent research into user experience has identified the need for a theoretical model to build cumulative knowledge in research addressing how the overall quality or 'goodness' of an interactive product is formed. An experiment tested and extended Hassenzahl's model of aesthetic experience. The study used a 2x2x(2) experimental design with three factors: principles of screen design, principles for organizing information on a web page and experience of using a web site. Dependent variables included hedonic perceptions and evaluations of a web site as well as measures of task performance, navigation behaviour and mental effort. Measures, except Beauty, were sensitive to manipulation of web design. Beauty was influenced by hedonic attributes (identification and stimulation), but Goodness by both hedonic and pragmatic (user-perceived usability) attributes as well as task performance and mental effort. Hedonic quality was more stable with experience of web-site use than pragmatic quality and Beauty was more stable than Goodness.",1
dedbeead-a96c-4cef-bfac-0243c404b733,42c64943-cd60-4d01-bf64-5e89d2982a46,"Modern epidemiology integrates knowledge from heterogeneous collections of data consisting of numerical, descriptive and imaging. Large-scale epidemiological studies use sophisticated statistical analysis, mathematical models using differential equations and versatile analytic tools that handle numerical data. In contrast, knowledge extraction from images and descriptive information in the form of text and diagrams remain a challenge for most fields, in particular, for diseases of the eye. In this article we provide a roadmap towards extraction of knowledge from text and images with focus on forthcoming applications to epidemiological investigation of retinal diseases, especially from existing massive heterogeneous collections of data distributed around the globe.","Most subfields of computer science have an interface layer via which applications communicate with the infrastructure, and this is key to their success (e.g., the Internet in networking, the relational model in databases, etc.). So far this interface layer has been missing in AI. First-order logic and probabilistic graphical models each have some of the necessary features, but a viable interface layer requires combining both. Markov logic is a powerful new language that accomplishes this by attaching weights to first-order formulas and treating them as templates for features of Markov random fields. Most statistical models in wide use are special cases of Markov logic, and first-order logic is its infinite-weight limit. Inference algorithms for Markov logic combine ideas from satisfiability, Markov chain Monte Carlo, belief propagation, and resolution. Learning algorithms make use of conditional likelihood, convex optimization, and inductive logic programming. Markov logic has been successfully applied to problems in information extraction and integration, natural language processing, robot mapping, social networks, computational biology, and others, and is the basis of the open-source Alchemy system.",1
793c319f-2635-48f6-8cdd-5b113aed1616,7b043552-62fb-48ce-b936-3b7d1cd01d26,"Applying rotated and cyclic Q delayed (RCQD) modulation on the transmitter side improves the performance of the receiver in case of fading channel conditions along with erasures. However, the complexity of demapping in the receiver increases significantly to achieve this elevated performance. Many low complexity demapping solutions are available but few are actually implemented in hardware. Recently, new constellation rotation angles and associated simplified demapping technique with better performance for fading channel conditions along with erasure scenarios has been proposed. In this paper, we propose a novel demapper architecture model that exploits these latest simplifications. Moreover, a joint demapper implementation is proposed, which can demap the symbols from different constellations using full or simplified demapping algorithm to achieve best throughput in terms of LLR/sec. Compared to state-of-the-art implementations of RCQD demappers, significant hardware reductions are demonstrated encouraging the use of RCQD modulation in future wireless communication applications.","Understanding relationships between entities in a computer network is an important task in enterprise cyber-security. This paper presents a novel procedure for exploring similarity relationships in Netflow behaviour - activity over time. We demonstrate a two-stage procedure. First, a statistical model is used as a summary of raw data. Naturally, the parameters of such a model are subject to estimation uncertainty. The second stage develops a similarity metric that incorporates this uncertainty. Standard clustering procedures then become available. We illustrate the method using connection-based data derived from Netflow records, from a recently released public domain data set.",0
86798e31-2925-4d72-ad9b-13b4935504f8,56b47e7d-dd4b-4970-9071-63bfaf89607c,"Location service is an essential prerequisite for mobile wireless ad hoc networks (MANETs) in which the underlying routing protocol leverages physical location information of sender and receiver nodes. Fulfillment of this requirement is challenging partly due to the mobility and unpredictability of nodes in MANETs. Moreover, scalability and location information availability under various circumstances are also substantial factors in designing an effective location service paradigm. By and large, utilizing centralized or distributed location servers responsible for storing the location information of all, or a subset of participant mobile devices, is a method employed in a significant portion of location service schemes. However, from the fairness point of view, it is more suitable to employ a location service scheme that treats participant nodes fairly, without mandating an unlucky subset to undertake the responsibility of serving as location server(s). In this work, we propose a scalable and fully decentralized location service scheme (PETAL) in which the burden of location update and inquiry tasks is almost evenly distributed among the nodes, resulting in an improvement in resilience against individual node failures. PETAL does not require hashing which results in more complexity, it is resilient against swarm mobility pattern, it requires minimal periodic location update messages when nodes do not move, and finally it does not require too many parameter configurations on all nodes. Our simulation results reveal that PETAL performs efficiently, particularly in environments densely populated by wireless devices.","We consider a system where updates are generated randomly and transmitted to a monitor. Only a single update can be in the queue at a time and hence the source needs to prioritize between the current update being served or the incoming one. With Poisson arrival of updates, this choice gives rise to two transmission policies for the M/G/1/1 queue: preempting the current update or discarding the new one. We start by studying the average status update age as well as the optimal update arrival rate for these two schemes under general service time distribution. We then apply these results on two practical scenarios: updates are sent through a symbol erasure channel using (a) an infinite incremental redundancy (IIR) ARQ system and (b) a fixed redundancy (FR) ARQ system. In IIR, the transmission of an update continues until k_s unerased symbols are received. In the FR system, the update is divided into k_p packets encoded ratelessly and each packet is encoded using an (n_s,k_s)-Maximum Distance Separable (MDS) code. We show that in both schemes the best strategy would be not to preempt. Moreover, we also prove that, from an age point of view, IIR is better than FR.",0
911f6dfd-a9fd-41d9-89a5-143f2a43cf57,96b245c2-47a5-4aec-89f0-d2a362124845,"In a multirate wireless network, low data rate nodes consume proportionately more channel resources than high data rate nodes, resulting in low overall network performance. The use of multiple non-overlapping frequency channels in multirate wireless networks can overcome the performance degradation by having nodes communicate on different channels based on their data rates. However, no effort has been invested to utilize the multiple channels for a multirate wireless network. In this paper, we introduce the data rate adaptive channel assignment (DR-CA) algorithm for a multichannel multirate single-hop wireless network to provide higher network throughput and network efficiency. The main idea is to assign links having same or comparable data rates on the same channel to minimize the wastage of channel resources due to interference between high data links and low data rate links. We also design a new intermediary multichannel layer (IML) which resides between network layer and link layer, at which we implement the DR-CA algorithm. The IML design requires no modifications to the underlying MAC layer and upper layers of the network stack. To evaluate the proposed algorithm we define new performance metrics-channel efficiency for a multichannel multirate wireless network. Using OPNET simulations, we show that the multichannel enhancement using our proposed algorithm provides significant performance improvement in terms of network throughput and channel efficiency over existing approaches in multirate wireless networks. Under heavy load condition, the channel efficiency using DR-CA algorithm reaches 90% of the maximum limit. To the best of our knowledge, this is the first work to utilize the benefits of multiple channels in the multirate wireless network environment.","The use of landmarks during the provision of directions can greatly improve drivers' route-following performance. However, the successful integration of landmarks within in-vehicle navigation systems is predicated on the acquisition and deployment of good quality landmarks, as defined by their visibility, uniqueness, permanence, location etc., and their accurate and succinct depiction on in-vehicle displays and during accompanying verbal messages. Notwithstanding the inherent variability in the quality and propensity of landmarks within the driving environment, attending to in-vehicle displays and verbal messages while driving can distract drivers and heighten their visual and cognitive workload. Furthermore, vocal utterances are transient and can be littered with paralinguistic cues that can influence a driver's interpretation of what is said. In this paper, a driving simulator study is described aiming to investigate the augmentation of landmarks during the head up provision of route guidance advice Ã¢â‚¬â€œ a potential solution to some of these problems. Twenty participants undertook four drives utilising a navigation system presented on a head up display (HUD) in which navigational instructions were presented as either: conventional distance-to-turn information; on-road arrows; or augmented landmarks (either an arrow pointing to the landmark or a box enclosing the landmark adjacent to the required turning). Participants demonstrated significant performance improvements while using the augmented landmark 'box' compared to the conventional distance-to-turn information, with response times and success rates enhanced by 43.1% and 26.2%, respectively. Moreover, there were significant reductions in eyes off-the-road time when using this approach, and it also attracted the lowest subjective ratings of workload. The authors conclude that there are significant benefits to augmenting landmarks during the head-up provision of in-car navigation advice.",0
0977ddfd-d01a-46cd-93e4-a101757dd1e0,64166868-cc56-4d33-aba9-1168b22131a1,"Self-reconfigurable modular robots (SRMRs) have recently attracted considerable attention because of their numerous potential applications in the real world. In this paper, we draw a comprehensive comparison among five different algorithms in path planning of a novel SRMR system called ACMoD through an environment comprised of various terrains in a static condition. The contribution of this work is that the reconfiguration ability of ACMoD has been taken into account. This consideration, though raises new algorithmic challenges, equips the robot with new capability to pass difficult terrains rather than bypassing them, and consequently the robot can achieve better performance in terms of traversal time and energy consumption. In this work, four different optimization algorithms, including Adaptive Genetic Algorithm, Elitist Ant System, Dijkstra and Dynamic Weighting A*, along with a well-known reinforcement learning algorithm called Q-Learning, are proposed to solve this path planning problem. The outputs of these algorithms are the optimal path through the environment and the associated configuration on each segment of the path. The challenges involved in mapping the path planning problem to each algorithm are discussed in full details. Eventually, all algorithms are compared in terms of the quality of their solutions and convergence rate.","While GPUs play an increasingly important role in todayÃ¢  s high-performance computers, optimizing GPU performance continues to impose large burdens upon programmers. A major challenge in optimizing codes for GPUs stems from the two levels of hardware parallelism, blocks and threads; each of these levels has significantly different characteristics, requiring different optimization strategies.   In this paper, we propose a novel compiler optimization algorithm for GPU parallelism. Our approach is based on the polyhedral model, which has enabled significant advances in program analysis and transformation compared to traditional AST-based frameworks. We extend polyhedral schedules to enable two-level parallelization through the idea of superposition, which integrates separate schedules for block-level and thread-level parallelism. Our experimental results demonstrate that our proposed compiler optimization framework can deliver 1.8ï¿_  and 2.1ï¿_  geometric mean improvements on NVIDIA Tesla M2050 and K80 GPUs, compared to a state-of-the-art polyhedral parallel code generator (PPCG) for GPGPUs.",0
80601fe5-1f1f-4b83-926a-291ebc28d918,8b9c9328-2ba0-45a5-ae13-9f1cde8c8b43,"In recent work on the safety analysis of systems we have shown how causal relationships amongst events can be algorithmically inferred from probabilistic counterexamples and subsequently be mapped to fault trees. The resulting fault trees were significantly smaller and hence easier to understand than the corresponding probabilistic coun- terexample, but still contain all information needed to discern the causes for the occurrence of a hazard. More recently we have developed an ap- proach called Causality Checking which is integrated into the state-space exploration algorithms used for qualitative model checking and which is capable of computing causality relationships on-the-fly. The causality checking approach outperforms the probabilistic causality computation in terms of run-time and memory consumption, but can not provide a probabilistic measure. In this paper we combine the strengths of both approaches and propose an approach where the causal events are com- puted using causality checking and the probability computation can be limited to the causal events. We demonstrate the increase in performance of our approach using several case studies.","In recent years, several approaches to generate probabilistic counterexamples have been proposed. The interpretation of stochastic counterexamples, however, continues to be problematic since they have to be represented as sets of paths, and the number of paths in this set may be very large. Fault trees (FTs) are a well-established industrial technique to represent causalities for possible system hazards resulting from system or system component failures. In this paper we suggest a method to automatically derive FTs from counterexamples, including a mapping of the probability information onto the FT. We extend the structural equation approach by Pearl and Halpern, which is based on Lewis counterfactuals, so that it serves as a justification for the causality that our proposed FT derivation rules imply. We demonstrate the usefulness of our approach by applying it to an industrial case study.",1
a0991392-5953-4313-9110-f280d2c33519,411b08e8-59f1-44c4-b906-654d0bc7cd84,"Watermark detection is a way of verifying the existence of a watermark in a watermarking scheme used for copyright protection of digital data. Statistical modeling of wavelet subband coefficients has been extensively used in watermark detection. The effectiveness of a watermarking scheme depends directly on how the wavelet coefficients are modeled. It is known that the vector-based hidden Markov model (HMM) is a very powerful statistical model for describing the distribution of the wavelet coefficients, since it is capable of capturing the subband marginal distribution as well as the inter-scale and cross orientation dependencies of the wavelet coefficients. In this paper, it is shown that modeling using the vector-based HMM gives a better fit for the empirical data in comparison to modeling with Cauchy, Bessel-K form (BKF) and generalized Gaussian (GG) distributions. In view of this, we propose a locally-optimum blind watermark detector using the vector-based HMM in the wavelet domain. In a Bayesian framework, closed-form expressions for the mean and variance of a test statistic are derived, experimentally validated and used in evaluating the performance of the proposed detector. Using a number of test images, the performance of the proposed detector is evaluated. It is shown that the proposed detector provides a detection rate higher than that provided by other detectors designed based on the Cauchy, Gaussian, BKF or GG distributions for the wavelet coefficients. The proposed detector is also shown to be highly robust against various kinds of attacks.","Preserving news stories may be important because of various reasons like they provide detailed information about events and they may be used for research purposes in the long term. However, the news stories published online are in danger because of reasons like constant change in the technologies used to publish information and the formats for publication. Certain institutions or individuals may be interested in preserving news stories related to a particular event or topic. The stories should be collected from various online newspapers and preserved for the long term. The major issue in the preservation process is that newspapers use different formats for online publication of the stories. The paper presents a tool which is developed to addresses the issue. The tool facilitates users in the extraction of news stories from various online newspapers and migration to a normalized format.",0
f6d6c56e-ce28-4383-bd67-6b35ceec897e,92287f7b-e889-49d6-bfab-4dda5e7d7162,"Inspired by the evolution process of human intelligence and the social learning theory, a new swarm intelligence algorithm paradigm named the social learning optimization (SLO) algorithm is proposed. SLO consists of three co-evolution spaces: the bottom is the micro-space, where genetic evolution occurs; the middle layer is the learning space, where individuals enhance their intelligence through imitation learning and observational learning; knowledge is extracted from the middle layer and delivered to the top layer, which is called the belief space, where culture is established through knowledge accumulation and used to guide individuals' genetic evolution in the micro-space regularly. SLO is an optimization algorithm model for optimization problems, and a concrete algorithm could be generated by embodying SLO's three evolution spaces. Moreover, to demonstrate how to employ SLO and verify its superiority, this paper proposes the specific SLO (S-SLO) to solve the problem of QoS-aware cloud service composition. S-SLO is constructed by integrating the improved differential evolutionary (DE) algorithm and improved social cognitive optimization (SCO) into the micro-space and the learning space, respectively. Finally, experimental results and performance comparison show that the S-SLO is both effective and efficient. This work is expected to explore a novel swarm intelligence optimization model with better search capabilities and convergence rates, as well as to extend the theory of the swarm intelligence optimization algorithm.",,1
2444d58c-f321-4c33-8585-6e19af938a70,953506d2-2a97-41a6-843e-0c46db26597e,"The modern engineering design process often relies on computer simulations to evaluate candidate designs. This simulation-driven approach results in what is commonly termed a computationally expensive black-box optimization problem. In practise, there will often exist candidate designs which cause the simulation to fail. Such simulation failures can consume a large portion of the allotted computational resources, and thus can lead to search stagnation and a poor final solution. To address this issue, this study proposes a new computational intelligence optimization algorithm which combines a model and a k-NN classifier. The latter predicts which solutions are expected to cause the simulation to fail, and its prediction is incorporated with the model prediction to bias the search towards valid solutions, namely, for which the simulation is expected to succeed. A main contribution of this study is that to further improve the search efficacy, the proposed algorithm leverages on model-selection theory and continuously calibrates the classifier during the search. An extensive performance analysis using an engineering application of airfoil shape optimization shows the efficacy of the proposed algorithm.","When using computer simulations in engineering design optimization one often encounters vectors which â€˜crashâ€™ the simulation and so no fitness is associated with them. In this paper we refer to these as undefined vectors since the objective function is undefined there. Since each simulation run (a function evaluation) is expensive (anywhere from minutes to weeks of CPU time) only a small number of evaluations are allowed during the entire search and so such undefined vectors pose a risk of consuming a large portion of the optimization â€˜budgetâ€™ thus stalling the search. To manage this open issue we propose a classification-assisted framework for expensive optimization problems, that is, where candidate vectors are classified in a pre-evaluation stage whether they are defined or not. We describe: a) a baseline single-classifier framework (no undefined vectors in the model) b) a non-classification assisted framework (undefined vectors in the model) and c) an extension of the classifier-assisted framework to a multi-classifier setup. Performance analysis using a test problem of airfoil shape optimization shows: a) the classifier-assisted framework obtains a better solution compared to the non-classification assisted one and b) the classifier can data-mine the accumulated information to provide new insights into the problem being solved.",1
1a6af238-a8b1-4fe5-ae21-f243433ae694,978b708b-a2a2-4aaf-84c2-b81d68b0d03d,"This paper presents a low-power wakeup radio (WUR) for application in indoor location systems. The presented radio has a better performance than the state-of-the-art radios, since it has low-power consumption, only 10 Î_W and it is perfectly integrated into a wireless sensor network based on ZigBee, which is used for location purposes. This performance was achieved due to an optimised radio frequency design of the WUR, which was embedded with a low-cost/low-power processor, and due to an enhanced control algorithm. Moreover, a software defined radio approach has been used to implement the WUR protocol. The WUR was designed and integrated in an existing WSN-based indoor location system which was originally based on a periodic sleep-wake up duty-cycled protocol. In the WUR protocol the location sensor is kept in deep sleep mode until it receives an external wake-up order. According to estimates, with this scheme, the battery lifetime can be increased from 200 days (using conventional duty-cycle protocol) up to almost 8000 days (sensor waken-up few times per day).",,1
c163313a-8dd0-40cd-ba00-a348cccb8fc7,9c691a19-5b9e-4580-8376-633a250d5f60,"This paper proposes a simple linear Bayesian approach to reinforcement learning. We show that with an appropriate basis, a Bayesian linear Gaussian model is sufficient for accurately estimating the system dynamics, and in particular when we allow for correlated noise. Policies are estimated by first sampling a transition model from the current posterior, and then performing approximate dynamic programming on the sampled model. This form of approximate Thompson sampling results in good exploration in unknown environments. The approach can also be seen as a Bayesian generalisation of least-squares policy iteration, where the empirical transition matrix is replaced with a sample from the posterior.","There has been a lot of recent work on Bayesian methods for reinforcement learning exhibiting near-optimal online performance. The main obstacle facing such methods is that in most problems of interest, the optimal solution involves planning in an infinitely large tree. However, it is possible to obtain stochastic lower and upper bounds on the value of each tree node. This enables us to use stochastic branch and bound algorithms to search the tree efficiently. This paper proposes some algorithms and examines their complexity in this setting.",1
30b5ca38-582f-4bb4-8606-46ac5ac2da38,a3380485-1a5e-4789-94e3-1f1e4c0f7c55,"This study examined the effectiveness of an educational data mining method --Learning Factors Analysis (LFA) --on improving the learning efficiency in the Cognitive Tutor curriculum. LFA uses a statistical model to predict how students perform in each practice of a knowledge component (KC), and identifies over-practiced or under-practiced KCs. By using the LFA findings on the Cognitive Tutor geometry curriculum, we optimized the curriculum with the goal of improving student learning efficiency. With a control group design, we analyzed the learning performance and the learning time of high school students participating in the Optimized Cognitive Tutor geometry curriculum. Results were compared to students participating in the traditional Cognitive Tutor geometry curriculum. Analyses indicated that students in the optimized condition saved a significant amount of time in the optimized curriculum units, compared with the time spent by the control group. There was no significant difference in the learning performance of the two groups in either an immediate post test or a two-week-later retention test. Findings support the use of this data mining technique to improve learning efficiency with other computer-tutor-based curricula.","A cognitive model is a set of production rules or skills encoded in intelligent tutors to model how students solve problems. It is usually generated by brainstorming and iterative refinement between subject experts, cognitive scientists and programmers. In this paper we propose a semi-automated method for improving a cognitive model called Learning Factors Analysis that combines a statistical model, human expertise and a combinatorial search. We use this method to evaluate an existing cognitive model and to generate and evaluate alternative models. We present improved cognitive models and make suggestions for improving the intelligent tutor based on those models.",1
114f8eba-bba7-4aec-a11f-2ae73dd7eda2,afe6e3f5-b13f-402c-a4ac-04d58b95144d,"In this article we discuss automated preprocessing of environmental data for further use. Environmental data is by default heterogeneous, as it may consist of data from sources such as weather stations, weather radars, chemical sensors, acoustic sensors, and off-line laboratory analysis. When integrating data from such heterogeneous sources, it needs to be processed in a context dependent manner. In addition, there is no single generic processing method; rather, several atomic methods need to be applied and in an appropriate sequence. Furthermore, the problem is complicated by the requirements set by the intended use of the data. The requirements influence not only the set of applicable methods but also the application sequence. In this article, we study automation of the selection and sequencing of preprocessing methods based on the user requirements. As the main contribution, we propose here the use of characterizations and a reachability algorithm to solve the selection and sequencing problem. In this article, we present the algorithm and argue for its correctness. We also discuss, how the algorithm is implemented as a cloud service, and illustrate the use of the service with simple case studies. A characterization based method for automated preprocessing of environmental data.A formalization of the preprocessing selection and sequencing problem.An algorithm solving the selection and sequencing problem.Simple case study implementation as a cloud service.","Action Systems is a predicate transformer based formalism. It supports the development of provably correct reactive and distributed systems by refinement. Recently, Action Systems were extended with a differential action. It is used for modelling continuous behaviour, thus, allowing the use of refinement in the development of provably correct hybrid systems, i.e, a discrete controller interacting with some continuously evolving environment. However, refinement as a method is concerned with correctness issues only. It offers very little guidance in what details one should consider during the refinement steps to make the system more robust. That information is revealed by robustness analysis. Other formalisms not supporting refinement do have tool support for automating the robustness analysis, e.g., HyTech for linear hybrid automata. Consequently, we study in this paper the non-trivial translation problem between Action Systems and linear hybrid automata. As the main contribution, we give and prove correct an algorithm that translates a linear hybrid action system to a linear hybrid automaton. With this algorithm we combine the strengths of the two formalisms: we may use HyTech for the robustness analysis to guide the development by refinement.",1
e30697f9-6303-46ee-8067-f3a199cf0dce,bcb2df01-cbda-44b1-966f-8800718cefc1,"Constraint Handling Rules (CHR) is both an effective concurrent declarative constraint-based programming language and a versatile computational formalism. While conceptually simple, CHR is distinguished by a remarkable combination of desirable features: - a semantic foundation in classical and linear logic, - an effective and efficient sequential and parallel execution model - guaranteed properties like the anytime online algorithm properties - powerful analysis methods for deciding essential program properties. This overview of some CHR-related research and applications is by no means meant to be complete. Essential introductory reading for CHR provide the survey article (122 )a nd the books (55, 62). Up-to- date information on CHR can be found online at the CHR web- page www.constraint-handling-rules.org, including the slides of the keynote talk associated with this article. In addition, the CHR web- site dtai.cs.kuleuven.be/CHR/ offers everything you want to know about CHR, including online demo versions and free downloads of the language.","In this work, we consider the automatic generation of test inputs for Mercury programs. We use an abstract representation of a program that allows to reason about program executions as paths in a control-flow graph. Next, we define how such a path corresponds to a set of constraints whose solution defines input values for the predicate under test such that when the predicate is called with respect to these input values, the execution is guaranteed to follow the given path. The approach is similar to existing work for imperative languages, but has been considerably adapted to deal with the specificities of Mercury, such as symbolic data representation, predicate failure and non-determinism.",1
bbd5b33b-d38c-4627-b09e-d98024a59417,e405f07f-2f41-421b-92de-0917f087c024,"Recent years have seen an increasing interest in developing standards for linguistic annotation, with a focus on the interoperability of the resources. This effort, however, requires a profound knowledge of the advantages and disadvantages of linguistic annotation schemes in order to avoid importing the ï¬‚aws and weaknesses of existing encoding schemes into the new standards. This paper addresses the question how to compare syntactically annotated corpora and gain insights into the usefulness of speciï¬Åc design decisions. We present an exhaustive evaluation of two German treebanks with crucially different encoding schemes. We evaluate three different parsers trained on the two treebanks and compare results using EVALB, the Leaf-Ancestor metric, and a dependency-based evaluation. Furthermore, we present TePaCoC, a new testsuite for the evaluation of parsers on complex German grammatical constructions. The testsuite provides a well thought-out error classiï¬Åcation, which enables us to compare parser output for parsers trained on treebanks with different encoding schemes and provides interesting insights into the impact of treebank annotation schemes on speciï¬Åc constructions like PP attachment or non-constituent coordination.",,1
1af49d2f-710f-44d1-8ce2-de62bd2008d8,e4781082-3bad-46db-a78c-8f5eb0e85f3c,"Besides the fact that e-Assessment systems can efficiently conduct all paper based tests to evaluate knowledge and skills, they can offer a lot of new features via sophisticated information and communication technologies, including adaptive testing, immediate evaluation, etc. Most of the realised e-Assessment systems use pictures in the realisation of e-Testing, but the pictures are mainly used as supported media enhancement of the multiple choice questions. We introduce a brand new idea to use interactive images, where the user can navigate and zoom the picture and provide answers by clicking on an appropriate graphical object, mark a region, annotate, set an answer/comment on a given position, etc. The application domain of this innovation is huge, including e-Assessment for those sciences, where image analysis is essential, such as analysis of medical images, gathering map selective user's opinion etc. We have developed three new question types based on interactive images that offer these innovations. The impact is not just in enhancement of offered technology, but also on preventing various cheating methods, such as memorising, guessing, etc. These innovations can improve the assessment results, by a more correct evaluation and knowledge assessment.","Assessment is an important component of formal learning, and Computer Assisted Assessment (CAA) is a well established component of most online learning. However, technical issues such as interoperability and security, and pedagogic reservations as to its effectiveness still remain barriers to the uptake of CAA. In this paper we examine a number of current assessment projects, predominantly emanating from the UK, to consider how a service oriented architecture can facilitate the implementation of tailored assessment environments, providing improved assessments within an interoperable and secure framework.",1
a37f0fa1-8684-4e1f-84c9-55a53fca2252,eedcced8-3040-446e-9979-8d5882527fc7,"Locomotion requires longitudinal co-ordination. We have examined uni-directional synaptic coupling processes between two classes of neuronal network oscillators: autonomously active â€œintrinsicâ€ù oscillators, and â€œpotentialâ€ù oscillators that lack sufficient excitatory drive for autonomous activity. We model such oscillator networks in the bilaterally-symmetrical, Xenopus tadpole spinal cord circuits that co-ordinate swimming. â€œGlutamateâ€ù coupling EPSPs can entrain a second oscillator of lower frequency provided their strength is sufficient. Fast (AMPA) EPSPs advance spiking on each cycle, while slow (NMDA) EPSPs increase frequency over many cycles. EPSPs can also enable rhythmicity in â€œpotentialâ€ù oscillators and entrain them. IPSPs operate primarily on a cycle-by-cycle basis. They can advance or delay spiking to entrain a second â€œintrinsicâ€ù oscillator with higher, equal or lower frequency. Bilaterally symmetrical coupling connections operate twice per cycle: once in each half-cycle, on each side of the receiving oscillator. Excitatory and inhibitory coupling allow entrainment in complimentary areas of parameter space.","The use of computer simulations as a neurophysiological tool creates new possibilities to understand complex systems and to test whether a given model can explain experimental findings. Simulations, however, require a detailed specification of the model, including the nerve cell action potential and synaptic transmission. We describe a neuron model of intermediate complexity, with a small number of compartments representing the soma and the dendritic tree, and equipped with Na+, K+, Ca2+, and Ca2+ dependent K+ channels. Conductance changes in the different compartments are used to model conventional excitatory and inhibitory synaptic interactions. Voltage dependent NMDA-receptor channels are also included, and influence both the electrical conductance and the inflow of Ca2+ ions. This neuron model has been designed for the analysis of neural networks and specifically for the simulation of the network generating locomotion in a simple vertebrate, the lamprey. By assigning experimentally established properties to the simulated cells and their synapses, it has been possible to verify the sufficiency of these properties to account for a number of experimental findings of the network in operation. The model is, however, sufficiently general to be useful for realistic simulation also of other neural systems.",1
49df389e-c3d6-4be3-a116-a23310c55b17,f63c28f4-eca5-4856-9399-829ac885f4d9,"We investigate the complexity of axiom pinpointing for different members of the DL-Lite family of Description Logics. More precisely, we consider the problem of enumerating all minimal subsets of a given DL-Lite knowledge base that have a given consequence. We show that for the DL-LiteHcore, DL-LiteHkrom and DL-LiteHNhorn fragments such minimal subsets are efficiently enumerable with polynomial delay, but for the DL-Litebool fragment they cannot be enumerated in output polynomial time unless P = NP. We also show that interestingly, for the DL-LiteHNhorn fragment such minimal sets can be enumerated in reverse lexicographic order with polynomial delay, but it is not possible in the forward lexicographic order since computing the first one is already coNP-hard.","For ontologies represented as Description Logic Tboxes, optimised DL reasoners are able to detect logical errors, but there is comparatively limited support for resolving such problems. One possible remedy is to weaken the available information to the extent that the errors disappear, but to limit the weakening process as much as possible. The most obvious way to do so is to remove just enough Tbox sentences to eliminate the errors. In this paper we propose a tableau-like procedure for finding maximally concept-satisfiable terminologies represented in the description logic ALC. We discuss some optimisation techniques, and report on preliminary, but encouraging, experimental results.",1
299cff5a-acc4-4fd8-8f60-27e40e44dacd,f63c28f4-eca5-4856-9399-829ac885f4d9,"The framework developed in this paper can deal with scenarios where selected sub-ontologies of a large ontology are offered as views to users, based on criteria like the user's access right, the trust level required by the application, or the level of detail requested by the user. Instead of materializing a large number of different sub-ontologies, we propose to keep just one ontology, but equip each axiom with a label from an appropriate labeling lattice. The access right, required trust level, etc. is then also represented by a label (called user label) from this lattice, and the corresponding sub-ontology is determined by comparing this label with the axiom labels. For large-scale ontologies, certain consequence (like the concept hierarchy) are often precomputed. Instead of precomputing these consequences for every possible sub-ontology, our approach computes just one label for each consequence such that a comparison of the user label with the consequence label determines whether the consequence follows from the corresponding sub-ontology or not.#R##N##R##N#In this paper we determine under which restrictions on the user and axiom labels such consequence labels (called boundaries) always exist, describe different black-box approaches for computing boundaries, and present first experimental results that compare the efficiency of these approaches on large real-world ontologies. Black-box means that, rather than requiring modifications of existing reasoning procedures, these approaches can use such procedures directly as sub-procedures, which allows us to employ existing highly-optimized reasoners.","For ontologies represented as Description Logic Tboxes, optimised DL reasoners are able to detect logical errors, but there is comparatively limited support for resolving such problems. One possible remedy is to weaken the available information to the extent that the errors disappear, but to limit the weakening process as much as possible. The most obvious way to do so is to remove just enough Tbox sentences to eliminate the errors. In this paper we propose a tableau-like procedure for finding maximally concept-satisfiable terminologies represented in the description logic ALC. We discuss some optimisation techniques, and report on preliminary, but encouraging, experimental results.",1
7e056f87-d7a7-49d3-9e4a-138f62aa569f,f6de7421-7dbc-4be3-89aa-354ddb84384c,"Machine learning has been proven useful for solving the bottlenecks in building expert systems. Noise in the training instances will, however, confuse a learning mechanism. Two main steps are adopted here to solve this problem. The first step is to appropriately arrange the training order of the instances. It is well known from Psychology that different orders of presentation of the same set of training instances to a human may cause different learning results. This idea is used here for machine learning and an order arrangement scheme is proposed. The second step is to modify a conventional noise-free learning algorithm, thus making it suitable for noisy environment. The generalized version space learning algorithm is then adopted to process the training instances for deriving good concepts. Finally, experiments on the Iris Flower problem show that the new scheme can produce a good training order, allowing the generalized version space algorithm to have a satisfactory learning result.","This paper compares five methods for pruning decision trees, developed from sets of examples. When used with uncertain rather than deterministic data, decision-tree induction involves three main stagesâ€”creating a complete tree able to classify all the training examples, pruning this tree to give statistical reliability, and processing the pruned tree to improve understandability. This paper concerns the second stageâ€”pruning. It presents empirical comparisons of the five methods across several domains. The results show that three methodsâ€”critical value, error complexity and reduced errorâ€”perform well, while the other two may cause problems. They also show that there is no significant interaction between the creation and pruning methods.",1
0c7d039e-336f-4592-b525-7e0e220b838a,f59313c4-4c7e-47dc-9ba9-a8f28da7977c,"Multilinear systems of equations arise in various applications, such as numerical partial differential equations, data mining, and tensor complementarity problems. In this paper, we propose a homotopy method for finding the unique positive solution to a multilinear system with a nonsingular M-tensor and a positive right side vector. We analyze the method and prove its convergence to the desired solution. We report some numerical results based on an implementation of the proposed method using a predictionâ€“correction approach for path following.","Object class segmentation is a computer vision task which requires labeling each pixel of an image with the class of the object it belongs to. Deep convolutional neural networks (DNN) are able to learn and take advantage of local spatial correlations required for this task. They are, however, restricted by their small, fixed-sized filters, which limits their ability to learn long-range dependencies. Recurrent Neural Networks (RNN), on the other hand, do not suffer from this restriction. Their iterative interpretation allows them to model long-range dependencies by propagating activity. This property is especially useful when labeling video sequences, where both spatial and temporal long-range dependencies occur. In this work, a novel RNN architecture for object class segmentation is presented. We investigate several ways to train such a network. We evaluate our models on the challenging NYU Depth v2 dataset for object class segmentation and obtain competitive results.",0
3381ab79-6cba-4878-885c-023981d41621,d0487195-db14-4f46-bccf-15d0acf530c1,"While GPUs play an increasingly important role in todayÃ¢  s high-performance computers, optimizing GPU performance continues to impose large burdens upon programmers. A major challenge in optimizing codes for GPUs stems from the two levels of hardware parallelism, blocks and threads; each of these levels has significantly different characteristics, requiring different optimization strategies.   In this paper, we propose a novel compiler optimization algorithm for GPU parallelism. Our approach is based on the polyhedral model, which has enabled significant advances in program analysis and transformation compared to traditional AST-based frameworks. We extend polyhedral schedules to enable two-level parallelization through the idea of superposition, which integrates separate schedules for block-level and thread-level parallelism. Our experimental results demonstrate that our proposed compiler optimization framework can deliver 1.8ï¿_  and 2.1ï¿_  geometric mean improvements on NVIDIA Tesla M2050 and K80 GPUs, compared to a state-of-the-art polyhedral parallel code generator (PPCG) for GPGPUs.","Most of existing works on the tubes design optimization of concentric-tube robot (CTR) do not include the elastic stability in the optimization criteria. The only work which formulates the elastic stability in the objective function is based on scalarization method which is used in existing multi-objective design optimization. The objective function is formed by a set of weighted objective functions. The selection of the weights is crucial as the optimization results are greatly affected by them and could be misleading if these weights are improperly chosen. As an alternative optimization technique, we use Pareto grid-searching method to avoid this problem and allow a straightforward interpretation of the results following the selection criteria for the parameters to be optimized. This paper shows a three-tube CTR design based on Pareto grid-searching method in order to optimize the reachability and elastic stability of the CTR within a specific curvature range dedicated to the deep anterior brain tumor removal surgery.",0
a9509a21-16df-4fac-83d2-13a18a20a2f6,e93140f8-ff5c-45c0-abd2-57dd471f4ed0,"Remote Access Trojans (RATs) provide cyber criminals with unlimited access to infected endpoints. Using the victim's access privileges, they can access and steal sensitive business and personal data including intellectual property and, personally identifiable information. However due to attack evolution, targeted attacks utilize modified versions of known signatures, which means that IDS rules that only match the known signature can be bypassed. In this paper, we propose a semi supervised approach that uses ensemble based label propagation to discover infected RAT packets in large unlabeled data. Our approach is trained on a small sample of labeled instances that usually characterize massive network datasets. Our approach is implemented using Apache Spark where we are able to demonstrate the effective discovery of such Trojans in massive amounts of data. We compare our approach to traditional signature based intrusion detection systems and clearly show that our approach is promising in the domain of cyber security in predicting large sets of unlabeled data using few labeled samples.","The linear nearest neighbor (LNN) restriction, present in several current implementations of 1D and 2D quantum circuits, limits the interaction of qubits to those which are adjacent to each other. While there have been several proposals to optimize the process of achieving LNN compliance in 1D circuits, there are few proposals for 2D circuits. Here, we present a new perspective on this problem for 2D quantum circuits. We propose to see this as a multiobjective optimization problem with two objectives: minimizing the size of the 2D grid in the circuit, and minimizing the number of SWAP gates required to achieve LNN compliance. We present some preliminary results which show that these are two contradictory objectives. Since this is common in multiobjective problems, these results indicate that a multiobjective algorithm might be a suitable way to address this problem, since it would make considerations which currently available methods do not make.",0
927b6817-28d8-43bc-80df-c8b77f969969,d775f6bb-40fd-4c72-b184-1444f0a14b4f,"State-of-the-art object-based approaches to automatic plant classification for crop/weed discrimination are reported to work with typical classification rates of 80â€“90% under laboratory or restricted field conditions. Adapting their parameter sets and classifiers to match changing field situation is laborious, yet it is required for practical application. Pixel-based classification allows adjusting the classifier model in the field easily by adding a few marks to sample data; however, pixel-based classification of camera data for crop/weed discrimination is impractical, as pixel features lack descriptiveness. This paper contributes a multi-wavelength laser line profile (MWLP) system for scanning the plants and obtaining sensor data, yielding image-based 3D range data, matched spectral reflectance, and scattering data at multiple wavelengths for each pixel. Using these descriptive pixel features, pixel-based Bayesian classification for crop/weed discrimination requires very few field-specific label data, thus allowing In-Field-Labeling for classifier adaptation to specific field situations. For different field situations and two different crops (carrots (Daucus carota) and corn salad (Valerianella locusta)) the classification using spectral and 3D features applying classifiers generated from very few marks in sample data (i.e., with very little effort for labeling), was successfully demonstrated, thereby achieving misclassification rates comparable to the best literature values.","The classification of graphs is a key challenge within many scientific fields using graphs to represent data and is an active area of research. Graph classification can be critical in identifying and labelling unknown graphs within a dataset and has seen application across many scientific fields. Graph classification poses two distinct problems: the classification of elements within a graph and the classification of the entire graph. Whilst there is considerable work on the first problem, the efficient and accurate classification of massive graphs into one or more classes has, thus far, received less attention. In this paper we propose the Deep Topology Classification (DTC) approach for global graph classification. DTC extracts both global and vertex level topological features from a graph to create a highly discriminate representation in feature space. A deep feed-forward neural network is designed and trained to classify these graph feature vectors. This approach is shown to be over 99% accurate at discerning graph classes over two datasets. Additionally, it is shown to be more accurate than current state of the art approaches both in binary and multi-class graph classification tasks.",0
52b683ad-2c75-433a-bb64-b36e5c48fde1,072aea53-e281-47f7-8203-e7259fcf54f7,"We present a derivation of a control-flow analysis by abstract interpretation. Our starting point is a transition system semantics defined as an abstract machine for a small functional language in continuation-passing style. We obtain a Galois connection for abstracting the machine states by composing Galois connections, most notable an independent-attribute Galois connection on machine states and a Galois connection induced by a closure operator associated with a constituent-parts relation on environments. We calculate abstract transfer functions by applying the state abstraction to the collecting semantics, resulting in a novel characterization of demand-driven 0-CFA.","Two different methods of flow analysis are discussed, one a significant generalization of the other. It is shown that the two methods have significantly different intrinsic computational complexities. As an outgrowth of our observations it is shown that a feature of the programming language used by Dijkstra in A Discipline of Programming makes it unsuitable for compile-time type checking, thus suggesting that flow analysis is applicable to the design of programming languages, as well as to their implementation. It is also shown that program verification by the method of inductive assertions is very likely to lead to assertions whose lengths and proofs are not polynomially bounded in the size of the program being verified, even for very simple programs. This last observation casts further doubt on the practicality and relevance of mechanized verification of arbitrary programs.",1
3ff1bd13-39dd-40db-83ee-8791a4db0e2f,05104314-7968-49d4-ae5d-f250ad95ddb0,"Color-depth cameras (RGB-D cameras) have become the primary sensors in most robotics systems, from service robotics to industrial robotics applications. Typical consumer-grade RGB-D cameras are provided with a coarse intrinsic and extrinsic calibration that generally does not meet the accuracy requirements needed by many robotics applications (e.g., high accuracy 3D environment reconstruction and mapping, high precision object recognition and localization, ...). In this paper, we propose a human-friendly, reliable and accurate calibration framework that enables to easily estimate both the intrinsic and extrinsic parameters of a general color-depth sensor couple. Our approach is based on a novel, two components, measurement error model that unifies the error sources of different RGB-D pairs based on different technologies, such as structured-light 3D cameras and time-of-flight cameras. The proposed correction model is implemented using two different parametric undistortion maps that provide the calibrated readings by means of linear combinations of control functions. Our method provides some important advantages compared to other state-of-the-art systems: it is general (i.e., well suited for different types of sensors), it is based on an easy and stable calibration protocol, it provides a greater calibration accuracy, and it has been implemented within the ROS robotics framework. We report detailed and comprehensive experimental validations and performance comparisons to support our statements.","This article addresses a virtual machine (VM) allocation problem that appears in a novel business model for cloud computing. In this model, a cloud service broker owns a number of cloud reserved instances that outsources to its customers as cheap as on-demand VMs. The objective of the broker is to efficiently manage its reserved resources to maximize its revenue. We enhance the previous definition of the problem by considering more realistic parameters: geographical localization of resources and users, different types of applications, and data transfer costs. We propose a set of heuristics to solve the optimization problem of maximizing the cloud provider profit while offering appropriate Quality-of-Service to the users. The experimental analysis is performed over different scenarios using real data from cloud providers.",0
733347b5-4a02-4413-ac4c-93899ab336db,0a5893d8-c402-47c0-8cd6-ecb1a1841c77,"This paper studies the opinion dynamics that result when individuals consecutively discuss a sequence of issues. Specifically, we study how individuals' self-confidence levels evolve via a reflected appraisal mechanism. Motivated by the DeGroot-Friedkin model, we propose a Modified DeGroot-Friedkin model which allows individuals to update their self-confidence levels by only interacting with their neighbors and in particular, the modified model allows the update of self-confidence levels to take place in finite time without waiting for the opinion process to reach a consensus on any particular issue. We study properties of this Modified DeGroot-Friedkin model and compare the associated equilibria and stability with those of the original DeGroot-Friedkin model. Specifically, for the case when the interaction matrix is doubly stochastic, we show that for the modified model, the vector of individuals' self-confidence levels converges to a unique nontrivial equilibrium which for each individual is equal to 1 over n, where n is the number of individuals. This implies that eventually individuals reach a democratic state.","The process by which new ideas, innovations, and behaviors spread through a large social network can be thought of as a networked interaction game: Each agent obtains information from certain number of agents in his friendship neighborhood, and adapts his idea or behavior to increase his benefit. In this paper, we are interested in how opinions, about a certain topic, form in social networks. We model opinions as continuous scalars ranging from 0 to 1 with 1(0) representing extremely positive(negative) opinion. Each agent has an initial opinion and incurs some cost depending on the opinions of his neighbors, his initial opinion, and his stubbornness about his initial opinion. Agents iteratively update their opinions based on their own initial opinions and observing the opinions of their neighbors. The iterative update of an agent can be viewed as a myopic cost-minimization response (i.e., the so-called best response) to the others' actions. We study whether an equilibrium can emerge as a result of such local interactions and how such equilibrium possibly depends on the network structure, initial opinions of the agents, and the location of stubborn agents and the extent of their stubbornness. We also study the convergence speed to such equilibrium and characterize the convergence time as a function of aforementioned factors. We also discuss the implications of such results in a few well-known graphs including small-world graphs.",1
2d15aa43-e29b-48ad-9afc-02db50591eb8,0bc83749-8225-4f84-975c-5674e021dcf8,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. In this paper, we consider the problem of learning shared structures from multiple related tasks. We present an improved formulation ( i ASO) for multi-task learning based on the non-convex alternating structure optimization (ASO) algorithm, in which all tasks are related by a shared feature representation. We convert  i ASO, a non-convex formulation, into a relaxed convex one, which is, however, not scalable to large data sets due to its complex constraints. We propose an alternating optimization ( c ASO) algorithm which solves the convex relaxation efficiently, and further show that  c ASO converges to a global optimum. In addition, we present a theoretical condition, under which  c ASO can find a globally optimal solution to  i ASO. Experiments on several benchmark data sets confirm our theoretical analysis.","The sum of the largest eigenvalues of a symmetric matrix is a nonsmooth convex function of the matrix elements. Max characterizations for this sum are established, giving a concise characterization of the subdifferential in terms of a dual matrix. This leads to a very useful characterization of the generalized gradient of the following convex composite function: the sum of the largest eigenvalues of a smooth symmetric matrix-valued function of a set of real parameters. The dual matrix provides the information required to either verify first-order optimality conditions at a point or to generate a descent direction for the eigenvalue sum from that point, splitting a multiple eigenvalue if necessary. Connections with the classical literature on sums of eigenvalues and eigenvalue perturbation theory are discussed. Sums of the largest eigenvalues in the absolute value sense are also addressed.",1
6750378d-2b48-41e5-9ed4-7dcd7d21b122,1d94a763-f845-4b3f-b336-5a44255c7d23,"Characterizing the polynomial chaos expansion (PCE) of a vector-valued random variable with probability distribution concentrated on a manifold is a relevant problem in data-driven settings. The probability distribution of such random vectors is multimodal in general, leading to potentially very slow convergence of the PCE. In this paper, we build on a recent development for estimating and sampling from probabilities concentrated on a diffusion manifold. The proposed methodology constructs a PCE of the random vector together with an associated generator that samples from the target probability distribution which is estimated from data concentrated in the neighborhood of the manifold. The method is robust and remains efficient for high dimension and large datasets. The resulting polynomial chaos construction on manifolds permits the adaptation of many uncertainty quantification and statistical tools to emerging questions motivated by data-driven queries.","Min-Hash, as a member of the Locality Sensitive Hashing (LSH) family for sketching sets, plays an important role in the big data era. It is widely used for efficiently estimating similarities of bag-of-words represented data and has been extended to dealing with multi-sets and real-value weighted sets. Improved Consistent Weighted Sampling (ICWS) has been recognized as the state-of-the-art for real-value weighted Min-Hash. However, the algorithmic implementation of ICWS is flawed because it violates the uniformity of the Min-Hash scheme. In this paper, we propose a Canonical Consistent Weighted Sampling (CCWS) algorithm, which not only retains the same theoretical complexity as ICWS but also strictly complies with the definition of Min-Hash. The experimental results demonstrate that the proposed CCWS algorithm runs faster than the state-of-the-arts while achieving similar classification performance on a number of real-world text data sets.",0
7464d967-2f7d-4786-abd1-443059f64bb9,60468a2e-c259-497e-b6bf-98e68c48bf36,"Heterogeneous systems have increased their popularity in recent years due to the high performance and reduced energy consumption capabilities provided by using devices such as GPUs or Xeon Phi accelerators. This paper proposes a checkpoint-based fault tolerance solution for heterogeneous applications, allowing them to survive fail-stop failures in the host CPU or in any of the accelerators used. Besides, applications can be restarted changing the host CPU and/or the accelerator device architecture, and adapting the computation to the number of devices available during recovery. The proposed solution is built combining CPPC (ComPiler for Portable Checkpointing), an application-level checkpointing tool, and HPL (Heterogeneous Programming Library), a library that facilitates the development of OpenCL-based applications. Experimental results show the low overhead introduced by the proposal and prove its portability and adaptability benefits.","In the Internet of Things, tremendous data is generated by huge number of things and stored in globally distributed data repositories. In this respect, many IoT applications would need to find and get the data indirectly from the data repositories or directly from things. GS1, global leading standard organization, firstly presents the concept of Discovery Service (DS) to find the desired EPC Information Services (EPCIS) like data repositories for things. To realize GS1 DS, many works are proposed with considerations of performance and security issues. However, most works are only applied to the Inter-DS finding one of distributed Intra-DSes; thus deterioration in the Intra-DS actually finding desired data repositories may adversely affect entire DS. In this paper, we propose Oliot-Discovery Service (DS) which deals with performance as well as security issues by especially focusing on Intra-DS aspect. For this, we have designed and implemented the access control pursuing the fine-grained access control model and the two-layered storage architecture comprised of cache and main databases. Moreover, it supports not only indirect access to things' data through the data repositories but also direct access to things which would be also required for many IoT applications. In order to show feasibility of Oliot-DS, we have constructed experimental test-bed and evaluated the performance. The experiment results show that Oliot-DS provides durability against many user transactions, and the reasonable write and read performances by preventing unauthorized access.",0
cc5038dc-75dd-40e8-9372-595d8edac607,17fb843a-827b-4578-859a-664f136dd586,"In recent years, advances in microelectronics have allowed the development of low-power, low-cost, and small devices with sensing, computing, and wireless communication capabilities. An important criterion on the design and operation of these devices is their energy consumption. Two factors that affect the energy consumption of the sensor network are computation and communication. The simple strategy of locally processing the acquired data and transmitting it, is not optimal from the energy consumption point of view, relative to other strategies based on distributed processing and successive transmission of partially processed data. In this paper, we analyze the energy tradeoff between computation and communication resources in order to minimize the total energy consumption. We propose a heuristic solution to the stated sensor network problem for a multimedia application using Strong Arm RISC processors to perform a certain data compression algorithm.","This work aims at demonstrating the usefulness of exploiting novel image-processing tools for moving-object detection and classification in the context of an actual application involving the remote monitoring of a tourist site. The application concerns outdoor people counting for tourist-flow estimation in a constrained environment. The technical problems to be solved are concerned with: (a) the design and implementation of low-complexity background updating and change detection algorithms able to adapt themselves to the time-varying illumination scene conditions, and (b) the integration of real-time pattern-recognition tools in order to distinguish group of persons to be counted from other objects present in the scene. The achieved results have proven that the proposed system makes it possible to obtain reliable people counting in different environmental situations, with an absolute mean error at most equal to 10%.",1
05de0628-5f0d-4c54-a8df-986c135d0a7e,57d658af-322f-4f0e-a513-233d18d4a0c6,"Time-Triggered Ethernet (TTEthernet) improved from Avionics Full Duplex Switched Ethernet (AFDX) is supposed to enhance the determination by introducing time-triggered mechanism, while indeterminacy problem still exists in rate-constrained (RC) flows thus requiring worst-case latency analysis. Traditional network calculus model used for AFDX is not suitable for TTEthernet due to time-triggered (TT) flow mandatory competition for network resource. In this paper, we derive formula of traffic delay for RC flows in TTEthernet with intensive TT-schedule schema under the condition of preemption transmission mode, expand performance analysis scope to support multi-priority flows through assuming TT flow as the highest-priority RC flow when considering the pessimism of scheduling and verify the practicability of modified model by calculating its algorithmic complexity. Experiments show that our model gives effective latency upper bound which is slightly beyond exact value but no more than 16.7% and sustains rapid calculation for a real complicated network configuration within 3 seconds compared with other existing models.","We study performance limits of solutions to utility maximization problems (e.g., max-min problems) in wireless networks as a function of the power budget $\\bar{p}$ available to transmitters. Special focus is devoted to the utility and the transmit energy efficiency (i.e., utility over transmit power) of the solution. Briefly, we show tight bounds for the general class of network utility optimization problems that can be solved by computing conditional eigenvalues of standard interference mappings. The proposed bounds, which are based on the concept of asymptotic functions, are simple to compute, provide us with good estimates of the performance of networks for any value of $\\bar{p}$ in many real-world applications, and enable us to determine points in which networks move from a noise limited regime to an interference limited regime. Furthermore, they also show that the utility and the transmit energy efficiency scales as $\\Theta(1)$ and $\\Theta(1/\\bar{p})$, respectively, as $\\bar{p}\\to\\infty$.",0
3c479ae9-db60-4b1f-b53d-b0c10588d337,c7c03b82-be33-44b0-b0ef-521942495ec7,"Many approaches have been proposed for human pose estimation in single and multi-view RGB images. However, some environments, such as the operating room, are still very challenging for state-of-the-art RGB methods. In this paper, we propose an approach for multi-view 3D human pose estimation from RGB-D images and demonstrate the benefits of using the additional depth channel for pose refinement beyond its use for the generation of improved features. The proposed method permits the joint detection and estimation of the poses without knowing a priori the number of persons present in the scene. We evaluate this approach on a novel multi-view RGB-D dataset acquired during live surgeries and annotated with ground truth 3D poses.","Flexible IT landscapes and efficient management of resources are key issues for enterprises. In cloud computing, varying requirements are discussed at application and platform level. In contrast, variability at cloud infrastructure level is seldom discussed. We have identified several variability requirements at infrastructure level from customer perspective. From service provider perspective, we also highlight which technical means needed to be employed to manage the variability. For elaboration, we provide examples for variability requirements and solutions. We also show with the help of a case study that how we can provide and manage variability using the software configuration tool Puppet. In the end, we summarize our paper and provide an outlook for future work.",0
24a46713-b82f-45ef-91f4-92f4200b20e9,1c7782d6-edec-440d-bc58-d34c212d1d04,"Meeting clock skew constraint is one of the most important tasks in the synthesis of clock trees. Moreover, the problem becomes much hard to tackle as the delay of clock signals varies dynamically during execution. Recently, it is shown that adjustable delay buffer (ADB) whose delay can be adjusted dynamically can solve the clock skew variation problem effectively. However, inserting ADBs requires non-negligible area and control overhead. Thus, all previous works have invariably aimed at minimizing the number of ADBs to be inserted, particularly under the environment of multiple power modes in which the operating voltage applied to some modules varies as the power mode changes. In this work, unlike the previous works which have solved the ADB minimization problem heuristically or locally optimally, we propose an elegant and easily adoptable solution to overcome the limitation of the previous works. Precisely, we propose  an  O( n  log  n )  time  (bottom-up traversal)  algorithm that  (1)  optimally solves the problem of minimizing the number of ADBs to be inserted with continuous delay of ADBs  and (2)  enables solving the ADB insertion problem with discrete delay of ADBs to be greatly simple and predictable . In addition, we propose (3)  a systematic solution to  an important extension to  the problem of buffer sizing combined with the ADB insertion  to further reduce the ADBs to be used.","For the cost-effective implementation of clock trees in through-silicon via (TSV)-based 3D IC designs, we propose core algorithms for 3D clock tree synthesis. For a given abstract tree topology, we propose DLE-3D (deferred layer embedding for &3Dlowbar; ICs), which optimally finds the embedding layers of tree nodes, so that the TSV cost required for a tree topology is minimized, and DME-3D (deferred merge embedding for &3Dlowbar; ICs), which is an extended algorithm of the 2D merging segment, to minimize the total wirelength in 3D design space, with the consideration of the TSV effect on delay. In addition, when an abstract tree topology is not given, we propose NN-3D (nearest neighbor selection for &3Dlowbar; ICs), which constructs a (TSV and wirelength) cost-effective abstract tree topology for 3D ICs. Through experimentation, we have confirmed that the clock tree synthesis flow using the proposed algorithms is very effective, outperforming the existing 3D clock tree synthesis in terms of the number of TSVs, total wirelength, and clock power consumption.",1
40b7a55f-f49e-47b4-ab44-e8ba3892a84c,1e2ec571-10e5-44da-b432-cd1b00745551,"Although non-English-speaking online populations are growing rapidly, support for searching non-English Web content is much weaker than for English content. Prior research has implicitly assumed English to be the primary language used on the Web, but this is not the case for many non-English-speaking regions. This research proposes a language-independent approach that uses meta-searching, statistical language processing, summarization, categorization, and visualization techniques to build high-quality domain-specific collections and to support searching and browsing of non-English information. Based on this approach, we developed SBizPort and AMedPort for the Spanish business and Arabic medical domains respectively. Experimental results showed that the portals achieved significantly better search accuracy, information quality, and overall satisfaction than benchmark search engines. Subjects strongly favored the portals' search and browse functionality and user interface. This research thus contributes to developing and validating a useful approach to non-English Web searching and providing an example of supporting decision-making in non-English Web domains.","A framework for constructing a cognitive model of usersâ€™ information searching behaviour is described. The motivation for the framework is to create explanatory and predictive theories of information searching to improve the design of information retrieval (IR) systems. The framework proposes a taxonomy of components for process models of the information seeking task, information need types and knowledge sources necessary to support the task. The framework is developed into a preliminary version of a cognitive theory of information searching by the addition of strategies and correspondence rules which predict user behaviour in different task stages according to information need types, facilities provided by the IR system and knowledge held by the user. The theory is evaluated by using claims analysis based on empirical observations of users information retrieval and by a walkthrough of an IR session to investigate how well the theory can account for empirical evidence. Results show that the theory can indicate the expert strategies which should be followed in different task contexts but predictions of actual user behaviour are less accurate. The future possibilities for employing the theoretical model as a tutorial advisor for information retrieval and as an evaluation method for IR systems are reviewed. The role and potential of cognitive theories of user task-action in Information Retrieval and Human Computer Interaction are discussed. 0 1997 Elsevier Science B.V.",1
09eddccf-30cd-4779-a2b2-5f5a15752c13,1fd20c6f-9212-4410-b7a0-9bcdaee9c858,"We study the problem of indexing text with wildcard positions, motivated by the challenge of aligning sequencing data to large genomes that contain millions of single nucleotide polymorphisms (SNPs)--positions known to differ between individuals. SNPs modeled as wildcards can lead to more informed and biologically relevant alignments. We improve the space complexity of previous approaches by giving a succinct index requiring (2 + o(1))n log Ïƒ + O(n) + O(d log n) + O(k log k) bits for a text of length n over an alphabet of size Ïƒ containing d groups of k wildcards. The new index is particularly favourable for larger alphabets and comparable for smaller alphabets, such as DNA. A key to the space reduction is a result we give showing how any compressed suffix array can be supplemented with auxiliary data structures occupying O(n) + O(d log n/d) bits to also support efficient dictionary matching queries. We present a new query algorithm for our wildcard index that greatly reduces the query working space to O(dm + m log n) bits, where m is the length of the query. We note that compared to previous results this reduces the working space by two orders of magnitude when aligning short read data to the Human genome.","We present a succinct representation of a set of  n  points on an  n  Ã— n  grid using $n\\lg n + o(n\\lg n)$ bits to support orthogonal range counting in $O(\\lg n /\\lg\\lg n)$ time, and range reporting in $O(k\\lg n/\\lg\\lg n)$ time, where  k  is the size of the output. This achieves an improvement on query time by a factor of $\\lg\\lg n$ upon the previous result of Makinen and Navarro [1], while using essentially the information-theoretic minimum space. Our data structure not only can be used as a key component in solutions to the general orthogonal range search problem to save storage cost, but also has applications in text indexing. In particular, we apply it to improve two previous space-efficient text indexes that support substring search [2] and position-restricted substring search [1]. We also use it to extend previous results on succinct representations of sequences of small integers, and to design succinct data structures supporting certain types of orthogonal range query in the plane.",1
be2ce419-842a-43e8-979c-a359b7eba3c2,234d2ce4-978a-4cf6-92a7-5e5f9fb5ed1a,"The software systems have been exposed to constant changes in a short period of time. The evolution of these systems demands a trade-off among several attributes to keep the software quality acceptable. It requires high maintainable systems and makes maintainability one of the most important quality attributes. This paper approaches the system evolution through the analysis of potential new architectures using the evaluation of maintainability level. The goal is to relate maintainability metrics applied in the source-code of OO systems, in particular CCC, to notations defined by COSMIC methods and proposes metrics-based models to assess CCC in software architectures.","This empirical research was undertaken as part of a multi-method programme of research to investigate unsupported claims made of object-oriented technology. A series of subject-based laboratory experiments, including an internal replication, tested the effect of inheritance depth on the maintainability of object-oriented software. Subjects were timed performing identical maintenance tasks on object-oriented software with a hierarchy of three levels of inheritance depth and equivalent object-based software with no inheritance. This was then replicated with more experienced subjects. In a second experiment of similar design, subjects were timed performing identical maintenance tasks on object-oriented software with a hierarchy of five levels of inheritance depth and the equivalent object-based software.",1
a2238f9b-1ea1-49ab-81a0-bcd340e9e6f7,9ff4a38b-e7ec-4452-98b5-a539c06a7ec3,"The popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. In this paper, we present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about shared photographic images. We investigate this task using training data derived from image-grounded conversations on social media and introduce a new dataset of crowd-sourced conversations for benchmarking progress. Experiments using deep neural network models trained on social media data show that the combination of visual and textual context can enhance the quality of generated conversational turns. In human evaluation, a gap between human performance and that of both neural and retrieval architectures suggests that IGC presents an interesting challenge for vision and language research.","The P300 speller is a brain-computer interface that enables people with neuromuscular disorders to communicate based on eliciting event-related potentials (ERP) in electroencephalography (EEG) measurements. One challenge to reliable communication is the presence of refractory effects in the P300 ERP that induces temporal dependence in the user's EEG responses. We propose a model for the P300 speller as a communication channel with memory. By studying the maximum information rate on this channel, we gain insight into the fundamental constraints imposed by refractory effects. We construct codebooks based on the optimal input distribution, and compare them to existing codebooks in literature.",0
9a8fe7a6-6635-4dab-ac59-05caf8bcd043,261632ba-f2b3-4c5b-a9ac-a38eddd7ac13,"This paper studies the effect of short utterances and noise on the performance of automatic speaker recognition. We focus on calibration aspects, and propose a calibration strategy that uses quality measures to model the calibration parameters. We carry out the proposed calibration by using simple Quality Measure Functions (QMFs) of duration and measured signal-to-noise-ratio from speech segments. We test the effectiveness of the approach using two databases, the development set of the I4U collaboration for the NIST Speaker Recognition Evaluation (SRE) 2012, and the evaluation test material of NIST SRE 2012 itself. In comparison with conventional linear calibration, results show that the proposed QMF approach successfully improves the system performance in terms of both discrimination and calibration. 2015 Elsevier B.V. All rights reserved.","An evaluation of the verification and calibration performance of a face recognition system based on inter-session variability modelling is presented. As an extension to calibration through linear transformation of scores, categorical calibration is introduced as a way to include additional information about images for calibration. The cost of likelihood ratio, which is a well-known measure in the speaker recognition field, is used as a calibration performance metric. The results obtained from the challenging mobile biometrics and surveillance camera face databases indicate that linearly calibrated face recognition scores are less misleading in their likelihood ratio interpretation than uncalibrated scores. In addition, the categorical calibration experiments show that calibration can be used not only to improve the likelihood ratio interpretation of scores, but also to improve the verification performance of a face recognition system.",1
056cddde-ae88-41c1-bc62-2d663b954df0,266b32ec-d19a-475b-8f21-bcea30fcfb21,"Purpose â€“ The purpose of this article is to alert researchers to software for web tracking of information seeking behaviour, and to offer a list of criteria that will make it easier to select software. A selection of research projects based on web tracking as well as the benefits and disadvantages of web tracking are also explored.Design/methodology/approach â€“ An overview of the literature, including clarification of key concepts, a brief overview of studies of web information seeking behaviour based on web tracking, identification of software used, as well as the strengths and shortâ€êcomings noted for web tracking is used as a background to the identification of criteria for the selection of web tracking software.Findings â€“ Web tracking can offer very valuable information for the development of websites, portals, digital libraries, etc. It, however, needs to be supplemented by qualitative studies, and researchers need to ensure that the tracking software will collect the data required.Research limitations...","Abstract#R##N##R##N#Research on Web searching is at an incipient stage. This aspect provides a unique opportunity to review the current state of research in the field, identify common trends, develop a methodological framework, and define terminology for future Web searching studies. In this article, the results from published studies of Web searching are reviewed to present the current state of research. The analysis of the limited Web searching studies available indicates that research methods and terminology are already diverging. A framework is proposed for future studies that will facilitate comparison of results. The advantages of such a framework are presented, and the implications for the design of Web information retrieval systems studies are discussed. Additionally, the searching characteristics of Web users are compared and contrasted with users of traditional information retrieval and online public access systems to discover if there is a need for more studies that focus predominantly or exclusively on Web searching. The comparison indicates that Web searching differs from searching in other environments.",1
40bdad0e-cca5-42bd-a9ed-9ea827f6d0ad,266b32ec-d19a-475b-8f21-bcea30fcfb21,"L'article explore les directions prises par la recherche ces cinq dernieres annees dans le domaine de l'indexation et du classement operes par les moteurs de recherche sur le Web. L'accent est mis sur les etudes qui tentent de caracteriser le Web en tant qu'environnement pour la recherche d'information et de determiner l'etendue de l'indexation realisee par les moteurs de recherche, ainsi que sur les etudes qui developpent et evaluent des techniques d'indexation et de recherche automatisees. D'autres questions liees a l'indexation et au classement sont mentionnees (indexation de citations, relevance feedback et elargissement de requete, recherche de pages liees, questions-reponses sur le Web), ainsi que la recherche concernant le comportement des utilisateurs.","Abstract#R##N##R##N#Research on Web searching is at an incipient stage. This aspect provides a unique opportunity to review the current state of research in the field, identify common trends, develop a methodological framework, and define terminology for future Web searching studies. In this article, the results from published studies of Web searching are reviewed to present the current state of research. The analysis of the limited Web searching studies available indicates that research methods and terminology are already diverging. A framework is proposed for future studies that will facilitate comparison of results. The advantages of such a framework are presented, and the implications for the design of Web information retrieval systems studies are discussed. Additionally, the searching characteristics of Web users are compared and contrasted with users of traditional information retrieval and online public access systems to discover if there is a need for more studies that focus predominantly or exclusively on Web searching. The comparison indicates that Web searching differs from searching in other environments.",1
920ed5bd-946b-4f7a-bac8-6b2f7cd8e7ca,26731f93-e26b-48e1-8616-f7d97e438240,"Let G be a connected graph of order n and independence number Î±. We prove that G has a spanning tree with average distance at most 23Î±, if niÂ_?2Î±-1, and at most Î±+2, if n>2Î±-1. As a corollary, we obtain, for n sufficiently large, an asymptotically sharp upper bound on the average distance of G in terms of its independence number. This bound, apart from confirming and improving on a conjecture of Graffiti [8], is a strengthening of a theorem of Chung [1], and that of Fajtlowicz and Waller [8], on average distance and independence number of a graph.","Abstract#R##N##R##N#The transmission of a graph or digraph G is the sum of all distances in G. Strict bounds on the transmission are collected and extended for several classes of graphs and digraphs. For example, in the class of 2-connected or 2-edge-connected graphs of order n, the maximal transmission is realized only by the cycle Cn. The independence of the transmission on the diameter or radius is shown. Remarks are also given about the NP-hardness of some related algorithmic problems.",1
9314bdbe-7202-474c-b07c-7b3ba4544fa1,2df9773f-2923-40a9-be29-988816fb7c0a,"Cryptographic types are a way to express cryptographic guarantees (of secrecy and integrity) in a type system for a network programming language. This allows some of these guarantees to be checked statically, before a network program executes. Where dynamic checks are required, these are represented at the source language level as dynamic type-checking, and are translated by the compiler to lower level cryptographic operations. Static checking has the important advantage that it provides static guarantees of the reliability of a network application. It can also help to avoid the unnecessary overhead of run-time cryptographic operations where communication is through a trusted medium (e.g., the OS kernel, or a trusted subnet). Cryptographic types can also be used to build application-specific security protocols, where type-checking in the lower layers of the protocol stack verifies security properties for upper layers. Cryptographic types are described formally using two process calculi: a simple but limited calculus of virtual and actual cryptographic operations, the sec-calculus; and a more sophisticated calculus of type-based cryptographic operations, the ec-calculus. Correctness is verified for a scheme for compiling type operations in the ec-calculus to cryptographic operations.","Statically typed programming languages allow earlier errorchecking, better enforcement of diciplined programming styles, and thegeneration of more efficient object code than languages where all typeconsistency checks are performed at run time. However, even instatically typed languages, there is often the need to deal with datawhose type cannot be determined at compile time. T o handlesuch situations safely, we propose to add a type Dynamic  whose values are pairs of a value v  and a type tag T  where  v has the type denoted by  T . Instances of Dynamic  are built with an explicittagging   construct and inspected with a type safe typecase  construct.  This paper explores the syntax, operational semantics, and denotational semantics of a simple language that includes the type Dynamic . We give examples of howdynamically typed values can be used in programming. Then we discuss anoperational semantics for our language and obtain a soundness theorem. Wepresent two formulations of the denotational semantics of this languageand relate them to the operational semantics. Finally, we consider theimplications of polymorphism and some implementation issues.",1
4ccc2de5-b146-4385-bd78-a5acc5a26f4d,745b1cd9-abc7-4b3a-afed-7fa9c375f27b,"Visual tracking is a popular research area in computer vision, which is very difficult to actualize because of challenges such as changes in scale and illumination, rotation, fast motion, and occlusion. Consequently, the focus in this research area is to make tracking algorithms adapt to these changes, so as to implement stable and accurate visual tracking. This paper proposes a visual tracking algorithm that integrates the scale invariance of SURF feature with deep learning to enhance the tracking robustness when the size of the object to be tracked changes significantly. Particle filter is used for motion estimation. The confidence of each particle is computed via a deep neural network, and the result of particle filter is verified and corrected by mean shift because of its computational efficiency and insensitivity to external interference. Both qualitative and quantitative evaluations on challenging benchmark sequences demonstrate that the proposed tracking algorithm performs favorably against several state-of-the-art methods throughout the challenging factors in visual tracking, especially for scale variation.","The classification of graphs is a key challenge within many scientific fields using graphs to represent data and is an active area of research. Graph classification can be critical in identifying and labelling unknown graphs within a dataset and has seen application across many scientific fields. Graph classification poses two distinct problems: the classification of elements within a graph and the classification of the entire graph. Whilst there is considerable work on the first problem, the efficient and accurate classification of massive graphs into one or more classes has, thus far, received less attention. In this paper we propose the Deep Topology Classification (DTC) approach for global graph classification. DTC extracts both global and vertex level topological features from a graph to create a highly discriminate representation in feature space. A deep feed-forward neural network is designed and trained to classify these graph feature vectors. This approach is shown to be over 99% accurate at discerning graph classes over two datasets. Additionally, it is shown to be more accurate than current state of the art approaches both in binary and multi-class graph classification tasks.",0
8672c7dc-f05d-488f-a2e1-c3d1fe47dcb0,31a846ee-b699-4023-be23-6f581cdd556c,"We are interested in developing real-time applications such as games or virtual prototyping that take advantage of the user full-body input to control a wide range of entities, from a self-similar avatar to any type of animated characters, including virtual humanoids with differing size and proportions. The key issue is, as always in real-time interactions, to identify the key factors that should get computational resources for ensuring the best user interaction efficiency. For this reason we first recall the definition and scope of such essential terms as immersion and presence, while clarifying the confusion existing in the fields of Virtual Reality and Games. This is done in conjunction with a short literature survey relating our interaction efficiency goal to key inspirations and findings from the field of Action Neuroscience. We then briefly describe our full-body real-time postural control with proactive local collision avoidance. The concept of obstacle spherification is introduced both to reduce local minima and to decrease the user cognitive task while interacting in complex environments. Finally we stress the interest of the egocentric environment scaling so that the user egocentric space matches the one of a height-differing controlled avatar.","In the research community, Collaborative Virtual Environment (CVE) developers usually refer to the terms awareness and feedback as something necessary to maintain a fluent collaboration when highly interactive task have to be performed. However, it is remarkable that few studies address the effect that including special kind of awareness has on the task performance and the user experience.   This paper proposes how to face the implementation of awareness in order to be taken into account early in the development of a CVE. In addition, it is also described an experiment that was carried out to evaluate the effect of providing some visual cues, showing that users tend to make more mistakes when they are not provided.",1
53580b06-e15e-482b-a4f0-5889c49cc6ba,3491feb2-bf31-4fde-b199-b652007cd999,"The model-checking problem for 1-safe Petri nets and linear-time temporal logic (LTL) consists of deciding, given a 1-safe Petri net and a formula of LTL, whether the Petri net satisfies the property encoded by the formula. This paper introduces a semidecision test for this problem. By a semidecision test we understand a procedure which may answer 'yes', in which case the Petri net satisfies the property, or 'don't know'. The test is based on a variant of the so called automata-theoretic approach to model-checking and on the notion of T-invariant. We analyse the computational complexity of the test, implement it using 2lp - a constraint programming tool, and apply it to two case studies. This paper is a (very) abbreviated version of [6].","We investigate extensions of temporal logic by connectives defined by finite automata on infinite words. We consider three different logics, corresponding to three different types of acceptance conditions (finite, looping, and repeating) for the automata. It turns out, however that these logics all have the same expressive power and that their decision problems are all PSPACE-complete. We also investigate connectives defined by alternating automata and show that they do not increase the expressive power of the logic or the complexity of the decision problem.",1
5d4f4003-cc59-410b-a348-125b62c79fb7,3491feb2-bf31-4fde-b199-b652007cd999,"In modular verification the specification of a module consists of two parts. One part describes the guaranteed behavior of the module. The other part describes the assumed behavior of the environment with which the module is interacting. This is called the assume-guarantee paradigm. Even when one specifies the guaranteed behavior of the module in a branching temporal logic, the assumption in the assume-guarantee pair concerns the interaction of the environment with the module along each computation, and is therefore often naturally expressed in linear temporal logic. In this paper we consider assume-guarantee specifications in which the assumption is given by an LTL formula and the guarantee is given by a CTL formula. Verifying modules with respect to such specifications is called the linear-branching model-checking problem. We apply automata-theoretic techniques to obtain a model-checking algorithm whose running time is linear in the size of the module and the size of the CTL guarantee, but doubly exponential in the size of the LTL assumption. We also show that the high complexity in the size of the LTL specification is inherent by proving that the problem is EXPSPACE-complete. The lower bound applies even if the branching temporal guarantee is restricted to be specified in /spl forall/CTL, the universal fragment of CTL.","We investigate extensions of temporal logic by connectives defined by finite automata on infinite words. We consider three different logics, corresponding to three different types of acceptance conditions (finite, looping, and repeating) for the automata. It turns out, however that these logics all have the same expressive power and that their decision problems are all PSPACE-complete. We also investigate connectives defined by alternating automata and show that they do not increase the expressive power of the logic or the complexity of the decision problem.",1
414bf0e4-e67e-4087-8cd1-7b295fed642f,35469633-b315-45d1-80dc-d8e8e14c1481,"The semantics of objects and transactions in database systems are investigated. User-defined predicates called  consistency assertions  are used to specify user programs. Three new correctness criteria are proposed. The first correctness criterion  consistency  is based solely on the users' specifications and admit nonserializable executions that are acceptable to the users. Integrity constraints of the database are maintained through consistency assertions. The second correctness criterion  orderability  is a generalization of view serializability and represents a weak notion of equivalence to a serial schedule. Finally, the third correctness criterion  strong order-ability  is introduced as a generalization of conflict serializability. Unlike consistency, the notions of orderability allow users to operate an isolation as maintenance of the integrity constrainst now becomes the responsibility of the database system.","When the only information available about transactions is syntactic information, serializability is the main correctness criterion for concurrency control. Serializability requires that the execution of each transaction must appear to every other transaction as a single atomic step (i.e., the execution of the transaction cannot be interrupted by other transactions). Many researchers, however, have realized that this requirement is unnecessarily strong for many applications and can significantly increase transaction response time. To overcome this problem, a new approach for controlling concurrency that exploits the semantic information available about transactions to allow controlled nonserializable interleavings has recently been proposed. This approach is useful when the cost of producing only serializable interleavings is unacceptably high. The main drawback of the approach is the extra overhead incurred by utilizing the semantic information. We examine this new approach in this paper and discuss its strengths and weaknesses. We introduce a new formalization for the concurrency control problem when semantic information is available about the transactions. This semantic information takes the form of transaction types, transaction steps, and transaction break-points. We define a new class of â€œsafeâ€ù schedules called relatively consistent (RC) schedules. This class contains serializable as well as nonserializable schedules. We prove that the execution of an RC schedule cannot violate consistency and propose a new concurrency control mechanism that produces only RC schedules. Our mechanism assumes fewer restrictions on the interleavings among transactions than previously introduced semantic-based mechanisms.",1
f1fe0684-6087-4afc-ba90-160b9f8c3e97,3852c9aa-581f-4c01-b583-9ebc4974f564,"Symmetric submodular functions are an important family of submodular functions capturing many interesting cases, including cut functions of graphs and hypergraphs. Maximization of such functions subject to various constraints receives little attention by current research, unlike similar minimization problems that have been widely studied. In this work, we identify a few submodular maximization problems for which one can get a better approximation for symmetric objectives than the state-of-the-art approximation for general submodular functions.#R##N##R##N#We first consider the problem of maximizing a non-negative symmetric submodular function f:2N â†’ R+ subject to a down-monotone solvable polytope P â_† [0, 1]N. For this problem, we describe an algorithm producing a fractional solution of value at least 0.432 c f(OPT), where OPT is the optimal integral solution. Our second result considers the problem maxlf(S)c vSv e kr for a non-negative symmetric submodular function f:2N â†’ R+. For this problem, we give an approximation ratio that depends on the value k/vNv and is always at least 0.432. Our method can also be applied to non-negative non-symmetric submodular functions, in which case it produces 1/e âˆ’ o(1) approximation, improving over the best-known result for this problem. For unconstrained maximization of a non-negative symmetric submodular function, we describe a deterministic linear-time 1/2-approximation algorithm. Finally, we give a [1 âˆ’ (1 âˆ’ 1/k)k âˆ’ 1]-approximation algorithm for Submodular Welfare with k players having identical non-negative submodular utility functions and show that this is the best possible approximation ratio for the problem.","We study the Minimum Submodular-Cost Allocation problem (MSCA). In this problem we are given a finite ground set V and k non-negative submodular set functions f1, ... , fk on V. The objective is to partition V into k (possibly empty) sets A1, ... , Ak such that the sum Î£i=1k fi(Ai) is minimized. Several well-studied problems such as the non-metric facility location problem, multiway-cut in graphs and hypergraphs, and uniform metric labeling and its generalizations can be shown to be special cases of MSCA. In this paper we consider a convexprogramming relaxation obtained via the Lovasz-extension for submodular functions. This allows us to understand several previous relaxations and rounding procedures in a unified fashion and also develop new formulations and approximation algorithms for related problems. In particular, we give a (1.5 - 1/k)-approximation for the hypergraph multiway partition problem. We also give a min{2(1-1/k), HÎ”}-approximation for the hypergraph multiway cut problem when Î” is the maximum hyperedge size. Both problems generalize the multiway cut problem in graphs and the hypergraph cut problem is approximation equivalent to the nodeweighted multiway cut problem in graphs.",1
6407be5a-409f-4709-b6d8-a6cabc418f38,393987a9-0612-426b-a107-bce37cc0a48e,"Several studies show that the use of landmarks is crucial for pedestrian navigations systems. Furthermore, there are well-defined landmark features like its visual salience. However, the assessment and the selection of landmarks especially in large scale indoor environments is rarely examined. We conducted a user study in the university of Regensburg and assessed the visual attraction of objects with an eye tracker.#R##N#Our finndings show that functional landmarks like doors and stairs are most likely to be looked at and named as a landmark. Moreover, we could prove that measuring visual salience only is not suffcient to identify landmarks with regard to the use in a pedestrian navigation system.","Navigation aids based on landmarks have been successfully investigated over the last years. There seems to be little doubt that navigation systems using landmarks clearly outperform those systems, relying only on distance specifications. Little, however, is known about the optimal selection of landmarks that are used to lead the user through an environment. This paper presents five methods to select landmarks that are visually and semantically distinguishable and which are suited to lead the user to his/her goal. The paper also discusses the lessons learned from the application of these methods. It shows the results that can be expected and it highlights also their possible traps and drawbacks.",1
017440d5-6ba8-422a-bd7a-d59e3b9a4e77,70d88ef5-1fe3-4df8-a9af-becc2bb17a75,"Container identification and recognition is still performed manually or in a semi-automatic fashion in multiple ports globally. This results in errors and inefficiencies in port operations. The problem of automatic container identification and recognition is challenging as the ISO standard only prescribes the pattern of the code and does not specify other parameters such as the foreground and background colors, font type and size, orientation of characters (horizontal or vertical) so on. Additionally, the corrugated surface of container body makes the two dimensional projection of the text on three dimensional containers slanted and jagged. We propose a solution in the form of an end-to-end pipeline that uses Region Proposals generated based on Connected Components for text detection in conjunction with Spatial Transformer Networks for text recognition. We demonstrate via our experimental results that the pipeline is reliable and robust even in situations when the code characters are highly distorted and outperforms the state-of-the-art results for text detection and recognition over the containers. We achieve text coverage rate of 100% and text recognition rate of 99.64%.","This paper investigates a full duplex wireless-powered two way communication networks, where two hybrid access points (HAPs) and a number of amplify and forward relays both operate in full duplex scenario. We use time switching (TS) and static power splitting (SPS) schemes with two way full duplex wireless-powered networks as a benchmark. Then, the new time division duplexing static power splitting (TDD SPS) and the full duplex static power splitting (FDSPS) schemes as well as a simple relay selection strategy are proposed to improve the system performance. For TS, SPS, and FDSPS, the best relay harvests energy using the received RF signal from HAPs and uses harvested energy to transmit signal to each HAP at the same frequency and time, therefore only partial self-interference (SI) cancellation needs to be considered in the FDSPS case. For the proposed TDD SPS, the best relay harvests the energy from the HAP and its self-interference. Then, we derive closed-form expressions for the throughput and outage probability for delay limited transmissions over Rayleigh fading channels. Simulation results are presented to evaluate the effectiveness of the proposed scheme with different system key parameters, such as time allocation, power splitting ratio, and residual SI.",0
3e4be5e3-96b2-40d3-a0bb-f8b99c38c71a,395d396d-35ae-4329-baa3-99aff43a3641,"Clustering-based software architecture recovery is an area that has received significant attention in the software engineering community over the years. Its key concept is the compilation and clustering of a system-wide graph that consists of source code entities as nodes, and source code relations as edges. However, the related research has mostly focused on investigating different clustering methods and techniques, and consequently there is limited work on addressing the question of what is a minimal set of relations that can be easily extracted from the systemâ€™s source code, and yet can be accurately used for extracting its architecture. In this paper, we report on results obtained from an architecture recovery case study we have conducted, by considering all possible combinations which can be generated from thirteen commonly used source code relations. We have examined the similarity of the extracted architectures obtained by using each different relation combination for different systems, against the corresponding architecture which is obtained by applying all thirteen relations and whch we consider as the ground truth architecture. For this purpose, we have also examined whether the use of all these thirteen relations is indeed adequate to yield a ground truth architecture, by applying this architecture extraction process on five large sofware systems for which their ground truth architecture has been independently established. The overall results of our study indicate that there is small set of relations for procedural systems, and another similar set for object oriented systems, that can be easily extracted from the source code and yet used to yield an architecture that is close to the ground truth architecture.","We present a performance evaluation conducted on a production supercomputer of the Intel Xeon Processor E5-2680v3, a twelve-core implementation of the fourth-generation Haswell architecture, and compare it with Intel Xeon Processor E5-2680v2, an Ivy Bridge implementation of the third-generation Sandy Bridge architecture. Several new architectural features have been incorporated in Haswell including improvements in all levels of the memory hierarchy as well as improvements to vector instructions and power management. We critically evaluate these new features of Haswell and compare with Ivy Bridge using several low-level benchmarks including subset of HPCC, HPCG and four full-scale scientific and engineering applications. We also present a model to predict the performance of HPCG and Cart3D within 5%, and Overflow within 10% accuracy.",0
0dde46d3-c56f-4862-8931-793ecb82df64,6c3710bb-32ec-4ccf-95d8-da077b792801,"Fault Tree Analysis (FTA) is a well-established and well-understood technique, widely used for dependability evaluation of a wide range of systems. Although many extensions of fault trees have been proposed, they suffer from a variety of shortcomings. In particular, even where software tool support exists, these analyses require a lot of manual effort. Over the past two decades, research has focused on simplifying dependability analysis by looking at how we can synthesise dependability information from system models automatically. This has led to the field of model-based dependability analysis (MBDA). Different tools and techniques have been developed as part of MBDA to automate the generation of dependability analysis artefacts such as fault trees. Firstly, this paper reviews the standard fault tree with its limitations. Secondly, different extensions of standard fault trees are reviewed. Thirdly, this paper reviews a number of prominent MBDA techniques where fault trees are used as a means for system dependability analysis and provides an insight into their working mechanism, applicability, strengths and challenges. Finally, the future outlook for MBDA is outlined, which includes the prospect of developing expert and intelligent systems for dependability analysis of complex open systems under the conditions of uncertainty.","Tuberculosis is one of the oldest diseases with a death rate of 1.5 million per year. Tuberculosis spreads from one person to another through Mycobacterium tuberculosis. This bacteria belongs to the family Mycobacteriaceae, genus Mycobacterium, member of the tuberculosis complex. Mycobacterium tuberculosis is an acid-fast, aerobic, rod-shaped bacteria, ranging from 2 to 4 Ã‚ Âµm in length and 0.2 to 0.5 Ã‚ Âµm in width. Tuberculosis spreads through infected people via sneezing, coughing, etc., with humans acting as the host for the bacteria. The genome of Mycobacterium tuberculosis H37Rv encodes 3906 proteins, of which 1055 are hypothetical proteins (HPs), wherein the functions of the proteins are unknown. The sequences of 1055 HPs of Mycobacterium tuberculosis were analyzed and the functions of 578 HPs were subsequently predicted with a high level of confidence. Several enzymes, transporters and binding proteins of 1055 HPs in M. tuberculosis were analyzed and potential targets were discovered which contribute to the overall survival of the bacteria. The analysis will be of relevance in understanding the mechanism of the bacteria and will prove to be beneficial in the discovery of new drugs.",0
98ee0cd6-fc14-4ff3-ba1a-352d85b7a283,4973a4c1-78cb-4487-a494-a9d8e10811cd,"Context#R##N#Software practitioners are often the primary source of information for software engineering research. They naturally produce information about their experiences of software practice, and the beliefs they infer from their experiences. Researchers must evaluate the quality and quantity of this information for their research.#R##N#Objective#R##N#To examine how concepts and methods from argumentation research can be used to study practitionersâ€™ evidence, inference and beliefs so as to better understand and improve software practice.#R##N#Method#R##N#We develop a preliminary framework and preliminary methodology, and use those to identify, extract and structure practitionersâ€™ evidence, inference and beliefs. We illustrate the application of the framework and methodology with examples from a practitioner's blog post.#R##N#Result#R##N#The practitioner uses (factual) stories, analogies, examples and popular opinion as evidence, and uses that evidence in defeasible reasoning to justify his beliefs and to rebut the beliefs of other practitioners.#R##N#Conclusion#R##N#The framework, methodology and examples could provide a foundation for software engineering researchers to develop a more sophisticated understanding of, and appreciation for, practitionersâ€™ defeasible evidence, inference and belief. Further work needs to automate (parts of) the methodology to support larger-scale application of the methodology.","There has been a recent and dramatic surge in the popularity of text messaging as a means of connecting with our social networks. The current research represents the first randomized controlled studies to directly compare both the social and emotional impact of social support provided in-person versus through text messaging. In two lab-based experiments, emerging adults completed a stressful task and were randomly assigned to receive emotional support either in-person, via text messaging, or no support at all. Support was provided by a close friend in experiment 1 (nÂ =Â 64), and by a similar-aged confederate in experiment 2 (nÂ =Â 188). In both experiments, in-person support was associated with significantly higher positive affect compared to text messaging. In-person support also led to greater satisfaction with support, but only in experiment 2. Overall, this research suggests that there may be emotional costs to a reliance on digital forms of social communication during times of stress.",0
9d439070-fcc0-4696-8600-3864f0a7619f,3a215e0a-4d71-4804-a86d-d7e918c20a98,"Application security traditionally strongly relies upon security of the underlying operating system. However, operating systems often fall victim to software attacks, compromising security of applications as well. To overcome this dependency, Intel SGX allows to protect application code against a subverted or malicious OS by running it in a hardware-protected enclave. However, SGX lacks support for generic trusted I/O paths to protect user input and output between enclaves and I/O devices. This work presents SGXIO, a generic trusted path architecture for SGX, allowing user applications to run securely on top of an untrusted OS, while at the same time supporting trusted paths to generic I/O devices. To achieve this, SGXIO combines the benefits of SGX's easy programming model with traditional hypervisor-based trusted path architectures. Moreover, SGXIO can tweak insecure debug enclaves to behave like secure production enclaves. SGXIO surpasses traditional use cases in cloud computing and digital rights management and makes SGX technology usable for protecting user-centric, local applications against kernel-level keyloggers and likewise. It is compatible to unmodified operating systems and works on a modern commodity notebook out of the box. Hence, SGXIO is particularly promising for the broad x86 community to which SGX is readily available.","We explore a programming approach for concurrency that synchronizes all accesses to shared memory by default. Synchronization takes place by ensuring that all program code runs inside atomic sections even if the program code has external side effects. Threads are mapped to atomic sections that a programmer must explicitly split to increase concurrency.   A naive implementation of this approach incurs a large amount of overhead. We show how to reduce this overhead to make the approach suitable for realistic application programs on existing hardware. We present an implementation technique based on a special-purpose software transactional memory system. To reduce the overhead, the technique exploits properties of managed, object-oriented programming languages as well as intraprocedural static analyses and uses field-level granularity locking in combination with transactional I/O to provide good scaling properties.   We implemented the synchronized-by-default (SBD) approach for the Java language and evaluate its performance for six programs from the DaCapo benchmark suite. The evaluation shows that, compared to explicit synchronization, the SBD approach has an overhead between 0.4% and 102% depending on the benchmark and the number of threads, with a mean (geom.) of 23.9%.",0
35856df1-d8ea-4ce8-ac41-c77adb5feea3,a7f7c809-d295-403d-a957-5b93c455c9ed,"In the last 20 years, most vehicles have seen a large increase in complexity of the electronic and software systems, e.g. the number of electronic control units (ECUs) increased to over 70 in many cars and connectivity to the Internet or over Bluetooth is possible. In combination, the increased complexity and a large attack surface due to the connectivity makes it necessary to investigate the security of each ECU as well as the overall vehicle architecture. Most of the security countermeasures that are implemented to date or will be implemented in the future require some kind of hardware support. This paper identifies which security mechanisms are already implemented, and which should be introduced in the future. It further discusses which parts of each mechanism require special hardware support.","Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.",0
43710f35-05d7-41c3-b8c1-faf2f13ca6d6,99c52db0-e715-49a6-bf57-9b18aeb2240c,"Planning of safe trajectories with interventions in both lateral and longitudinal dynamics of vehicles has huge potential for increasing the road traffic safety. Main challenges for the development of such algorithms are the consideration of vehicle nonholonomic constraints and the efficiency in terms of implementation, so that algorithms run in real time in a vehicle. The recently introduced Augmented CL-RRT algorithm is an approach that uses analytical models for trajectory planning based on the brute force evaluation of many longitudinal acceleration profiles to find collision-free trajectories. The algorithm considers nonholonomic constraints of the vehicle in complex road traffic scenarios with multiple static and dynamic objects, but it requires a lot of computation time. This work proposes a hybrid machine learning approach for predicting suitable acceleration profiles in critical traffic scenarios, so that only few acceleration profiles are used with the Augmented CL-RRT to find a safe trajectory while reducing the computation time. This is realized using a convolutional neural network variant, introduced as 3D-ConvNet, which learns spatiotemporal features from a sequence of predicted occupancy grids generated from predictions of other road traffic participants. These learned features together with hand-designed features of the EGO vehicle are used to predict acceleration profiles. Simulations are performed to compare the brute force approach with the proposed approach in terms of efficiency and safety. The results show vast improvement in terms of efficiency without harming safety. Additionally, an extension to the Augmented CL-RRT algorithm is introduced for finding a trajectory with low severity of injury, if a collision is already unavoidable.","Multilinear systems of equations arise in various applications, such as numerical partial differential equations, data mining, and tensor complementarity problems. In this paper, we propose a homotopy method for finding the unique positive solution to a multilinear system with a nonsingular M-tensor and a positive right side vector. We analyze the method and prove its convergence to the desired solution. We report some numerical results based on an implementation of the proposed method using a predictionâ€“correction approach for path following.",0
8f8a0c7b-b865-4f01-a9ed-58a0ee3f94a2,3a5a78ae-0d18-4f0e-a6a1-c646669977d1,"Pulse generator serves an important role in gated ring oscillator (GRO) based Time-to-Digital converters (TDC) to enable the ring oscillator for the input time difference between two reference timing events. As the resolution of TDC advances to a few picoseconds, the linearity of the pulse generator becomes increasingly important. This paper reviews and compare between pulse generators implemented using a XOR gate, SR latch, phase frequency detector (PFD) and time difference generator (TDG). Simulation results in standard 40nm CMOS technology shows that the TDG is the most suitable pulse generator since it has the best linearity. In terms of power and area, the dynamic PFD is a good alternative.","While GPUs play an increasingly important role in todayÃ¢  s high-performance computers, optimizing GPU performance continues to impose large burdens upon programmers. A major challenge in optimizing codes for GPUs stems from the two levels of hardware parallelism, blocks and threads; each of these levels has significantly different characteristics, requiring different optimization strategies.   In this paper, we propose a novel compiler optimization algorithm for GPU parallelism. Our approach is based on the polyhedral model, which has enabled significant advances in program analysis and transformation compared to traditional AST-based frameworks. We extend polyhedral schedules to enable two-level parallelization through the idea of superposition, which integrates separate schedules for block-level and thread-level parallelism. Our experimental results demonstrate that our proposed compiler optimization framework can deliver 1.8ï¿_  and 2.1ï¿_  geometric mean improvements on NVIDIA Tesla M2050 and K80 GPUs, compared to a state-of-the-art polyhedral parallel code generator (PPCG) for GPGPUs.",0
a8bd6877-3c11-4af3-987e-7b5951acf204,9ea94e61-18fd-4681-b330-eaa1a52c0496,"A non-local means (NLM) filter is a weighted average of a large number of non-local pixels with various image intensity values. The NLM filters have been shown to have powerful denoising performance, excellent detail preservation by averaging many noisy pixels, and using appropriate values for the weights, respectively. The NLM weights between two different pixels are determined based on the similarities between two patches that surround these pixels and a smoothing parameter. Another important factor that influences the denoising performance is the self-weight values for the same pixel. The recently introduced local James-Stein type center pixel weight estimation method (LJS) outperforms other existing methods when determining the contribution of the center pixels in the NLM filter. However, the LJS method may result in excessively large self-weight estimates since no upper bound is assumed, and the method uses a relatively large local area for estimating the self-weights, which may lead to a strong bias. In this paper, we investigated these issues in the LJS method, and then propose a novel local self-weight estimation methods using direct bounds (LMM-DB) and reparametrization (LMM-RP) based on the Baranchikâ€™s minimax estimator. Both the LMM-DB and LMM-RP methods were evaluated using a wide range of natural images and a clinical MRI image together with the various levels of additive Gaussian noise. Our proposed parameter selection methods yielded an improved bias-variance trade-off, a higher peak signal-to-noise (PSNR) ratio, and fewer visual artifacts when compared with the results of the classical NLM and LJS methods. Our proposed methods also provide a heuristic way to select a suitable global smoothing parameters that can yield PSNR values that are close to the optimal values.","Massive Multiple-Input-Multiple-Output (M-MIMO) system is a promising technology that offers to mobile networks substantial increase in throughput. In Time-Division Duplexing (TDD), the uplink training allows a Base Station (BS) to acquire Channel State Information (CSI) for both uplink reception and downlink transmission. This is essential for M-MIMO systems where downlink training pilots would consume large portion of the bandwidth. In densely populated areas, pilot symbols are reused among neighboring cells. Pilot contamination is the fundamental bottleneck on the performance of M-MIMO systems. Pilot contamination effect in antenna arrays can be mitigated by treating the channel estimation problem in angular domain where channel sparsity can be exploited. In this paper, we introduce a codebook that projects the channel into orthogonal beams and apply Minimum Mean-Squared Error (MMSE) criterion to estimate the channel. We also propose data-aided channel covariance matrix estimation algorithm for angular domain MMSE channel estimator by exploiting properties of linear antenna array. The algorithm is based on simple linear operations and no matrix inversion is involved. Numerical results show that the algorithm performs well in mitigating pilot contamination where the desired channel and other interfering channels span overlapping angle-of-arrivals.",0
4330c5ca-f9ec-4d16-b6c8-75e6edbcb4f1,334e0cd3-62c9-4bc0-8a6b-5e8867ecea63,"Dynamically reconfigurable architectures, such as NATURE, achieve high logic density and low reconfiguration latency compared to traditional field-programmable gate arrays. Unlike fine-grained NATURE, reconfigurable DSP block incorporated NATURE architecture achieves significant improvement in performance for mapping compute-intensive arithmetic operations. However, the DSP block fails to fully exploit the potential provided by the run-time reconfiguration. This paper presents a pipeline reconfigurable DSP architecture to target the NATURE platform that supports temporal logic folding. The proposed approach allows the DSP pipeline stages to be reconfigured independently such that different functions can be performed distinctively and individually at every clock interval during runtime. In addition, a multistage clock gating technique is also used in the design to minimize the power consumption. We also extend NanoMap tool for mapping circuits on NATURE platform to exploit the pipeline-level reconfigurability of our proposed DSP block to enable efficient resource sharing and area/power reduction. Simulation results on 13 benchmarks show that the proposed approach enables area-delay improvement of up to 3.6\\(\\times \\) compared to the fine-grained NATURE architecture. The proposed architecture also delivers 31.42% reduction in area and a maximum of 4.18\\(\\times \\) improvement in power-delay compared to existing NATURE architecture. We also observe an average improvement of 29 and 54.13% in performance and area when compared to commercial Xilinx Spartan-3A DSP platform, thereby allowing the designers to tune the circuit implementations for the area, power, or performance benefits.","The internet technology and cloud computing development are increasing the opportunity for small and medium enterprises (SMEs) to be able adopt information technology at an affordable and does not require an expensive investment. By using cloud computing. SMEs only need to subscribe a service from an application without need to invest in the purchase of software or expensive hardware such as servers and other network devices. Cloud computing is a technology that provides a service for users to be able to choose what services you want to use both Platform As a Service (PaaS) or Software As a Service (SaaS), the advantages of using cloud computing is the customers have the flexibility to choose what services to be used without the need of how much investment is required when trying to build a system for business that they do. This research will build an information system that is required by small and medium enterprises (SMEs) based cloud computing, the research is expected to appear a solution for small and medium enterprises (SMEs) in building an information system at an affordable cost but still in accordance with the needs of SME, and still make SMEs competitive to face global competition.",0
32aae4ae-9f7a-4ce7-87a2-46253e789968,5382e49b-7d48-4b09-bd59-57bfdd604061,"The widespread availability of electronic health records (EHRs) promises to usher in the era of personalized medicine. However, the problem of extracting useful clinical representations from longitudinal EHR data remains challenging. In this paper, we explore deep neural network models with learned medical feature embedding to deal with the problems of high dimensionality and temporality. Specifically, we use a multi-layer convolutional neural network (CNN) to parameterize the model and is thus able to capture complex non-linear longitudinal evolution of EHRs. Our model can effectively capture local/short temporal dependency in EHRs, which is beneficial for risk prediction. To account for high dimensionality, we use the embedding medical features in the CNN model which hold the natural medical concepts. Our initial experiments produce promising results and demonstrate the effectiveness of both the medical feature embedding and the proposed convolutional neural network in risk prediction on cohorts of congestive heart failure and diabetes patients compared with several strong baselines.","Envelope Tracking Power Amplifier (ET PA) is becoming prevalent because of its higher power efficiency (compared to its linear PAs) and higher linearity (compared to other high power-efficiency PAs such as Envelope Elimination and Restoration PAs and outphasing PAs). Of various building blocks in an ET PA, the supply modulator is the most critical block as the supply modulator has major effect on the overall performance of the ET PA, including bandwidth, linearity, power-efficiency, etc. In this paper, a comprehensive literature review on supply modulators for ET PAs is provided. Various supply modulator design architectures are discussed, and their advantages and drawbacks compared. The key design parameters and design trade-offs are also delineated. The reviews and discussions herein provide useful references and valuable insights to supply modulator designers.",0
4fc94672-09bd-4af9-904b-51a7e18d8cf2,2e9801ad-2022-4186-abd4-d09c225dc3a4,"Gamification aims to implement game features in non-game contexts, with the goal of increasing the motivation of individuals performing a specific task or set of tasks. The tasks themselves, can focus on cognitive behavior change (e.g., overcoming anxiety) or physical behavior change (e.g., overcoming a shoulder injury). Current gamification methods primarily serve as guidelines and principles for the design of gamified applications. Moreover, these methods often overlook the complexity of actually implementing the game features and do not consider the effects that game features have on individuals' ability to perform a target task. A knowledge gap exists in understanding the tradeoffs between the complexity of implementing a game feature and the impact it has on increasing individuals' motivation and performance on a particular task or set of tasks. This paper presents a method for evaluating the complexity of implementing game features and the physical effort required to perform the tasks of the application, with a specific focus on physically-interactive gamified applications. Designers will gain a fundamental understanding of how the implementation of specific game features, contributes toward the objective of the application. A case study is presented that focuses on physically-interactive gamified applications in a virtual environment. Empirical results measuring the effects of game features on participants' performance are presented, which provide evidence in support of the metrics proposed in this study. Knowledge gained from this work will inform designers on how to manage their resources more efficiently and predict possible design issues (e.g., not meeting the objective of the application) while creating gamification applications.","Generations of computer scientists and practitioners have worked under the assumption that computers will keep improving themselves: just wait a few years and Moore's Law will solve your scaling problems. This reliable march of electrical-engineering progress has sparked revolutions in the ways humans use computers and interact with the world and each other. But growth in computing power has protected outdated abstractions and encouraged layering even more abstractions, whatever the cost.",0
9c363d57-4fb2-4b99-9c2a-bfa9fa6a2088,d870d37d-3c00-4995-b70c-534a23bac0b0,"Container based virtualization is rapidly growing in popularity for cloud deployments and applications as a virtualization alternative due to the ease of deployment coupled with high-performance. Emerging byte-addressable, nonvolatile memories, commonly called Storage Class Memory or SCM, technologies are promising both byte-addressability and persistence near DRAM speeds operating on the main memory bus. These new memory alternatives open up a new realm of applications that no longer have to rely on slow, block-based persistence, but can rather operate directly on persistent data using ordinary loads and stores through the cache hierarchy coupled with transaction techniques. However, SCM presents a new challenge for container-based applications, which typically access persistent data through layers of block based file isolation. Traditional persistent data accesses in containers are performed through layered file access, which slows byte-addressable persistence and transactional guarantees, or through direct access to drivers, which do not provide for isolation guarantees or security. This paper presents a high-performance containerized version of byte-addressable, non-volatile memory (SCM) for applications running inside a container that solves performance challenges while providing isolation guarantees. We created an open-source container-aware Linux loadable Kernel Module (LKM) called Containerized Storage Class Memory, or CSCM, that presents SCM for application isolation and ease of portability. We performed evaluation using microbenchmarks, STREAMS, and Redis, a popular in-memory data structure store, and found our CSCM driver has near the same memory throughput for SCM applications as a non-containerized application running on a host and much higher throughput than persistent in-memory applications accessing SCM through Docker Storage or Volumes.","Nearly fifteen years ago, Google unveiled the generalized second price (GSP) auction. By all theoretical accounts including their own [Varian 14], this was the wrong auction --- the Vickrey-Clarke-Groves (VCG) auction would have been the proper choice --- yet GSP has succeeded spectacularly.#R##N##R##N#We give a deep justification for GSP's success: advertisers' preferences map to a model we call value maximization; they do not maximize profit as the standard theory would believe. For value maximizers, GSP is the truthful auction [Aggarwal 09]. Moreover, this implies an axiomatization of GSP --- it is an auction whose prices are truthful for value maximizers --- that can be applied much more broadly than the simple model for which GSP was originally designed. In particular, applying it to arbitrary single-parameter domains recovers the folklore definition of GSP. Through the lens of value maximization, GSP metamorphosizes into a powerful auction, sound in its principles and elegant in its simplicity.",0